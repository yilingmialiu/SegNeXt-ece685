2022-11-29 16:29:44,622 - mmseg - INFO - Multi-processing start method is `None`
2022-11-29 16:29:44,622 - mmseg - INFO - OpenCV num_threads is `<built-in function getNumThreads>
2022-11-29 16:29:44,667 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.8 (default, Apr 13 2021, 19:58:26) [GCC 7.3.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 2080 Ti
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.5, V11.5.119
GCC: gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-3)
PyTorch: 1.11.0+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.2-Product Build 20210312 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu102
OpenCV: 4.6.0
MMCV: 1.6.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMSegmentation: 0.24.1+
------------------------------------------------------------

2022-11-29 16:29:44,668 - mmseg - INFO - Distributed training: False
2022-11-29 16:29:44,984 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
ham_norm_cfg = dict(type='GN', num_groups=32, requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained=None,
    backbone=dict(
        type='MSCAN',
        embed_dims=[64, 128, 320, 512],
        mlp_ratios=[8, 8, 4, 4],
        drop_rate=0.0,
        drop_path_rate=0.3,
        depths=[3, 5, 27, 3],
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        init_cfg=dict()),
    decode_head=dict(
        type='LightHamHead',
        in_channels=[128, 320, 512],
        in_index=[1, 2, 3],
        channels=1024,
        ham_channels=1024,
        dropout_ratio=0.1,
        num_classes=5,
        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=1.0,
            class_weight=[0.5, 1, 1, 1, 1])),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'LandcoverDataset'
data_root = '/datacommons/carlsonlab/yl407/landcover_torchgeo/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=False),
    dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=1,
    train=dict(
        type='LandcoverDataset',
        data_root='/datacommons/carlsonlab/yl407/landcover_torchgeo/',
        img_dir='images',
        ann_dir='labels',
        split='splits/train_sub.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', reduce_zero_label=False),
            dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='LandcoverDataset',
        data_root='/datacommons/carlsonlab/yl407/landcover_torchgeo/',
        img_dir='images',
        ann_dir='labels',
        split='splits/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='LandcoverDataset',
        data_root='/datacommons/carlsonlab/yl407/landcover_torchgeo/',
        img_dir='images',
        ann_dir='labels',
        split='splits/train_sub.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_block=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0),
            head=dict(lr_mult=10.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=40000)
checkpoint_config = dict(by_epoch=False, interval=5000)
evaluation = dict(interval=5000, metric='mIoU')
find_unused_parameters = True
work_dir = './work_dirs/segnext.large.512x512.landcover.40k_weight_sub_nopre'
gpu_ids = [0]
auto_resume = False

2022-11-29 16:29:44,985 - mmseg - INFO - Set random seed to 1424203983, deterministic: False
2022-11-29 16:29:45,495 - mmseg - INFO - initialize LightHamHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

backbone.patch_embed1.proj.0.weight - torch.Size([32, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed1.proj.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed1.proj.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed1.proj.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed1.proj.3.weight - torch.Size([64, 32, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed1.proj.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed1.proj.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed1.proj.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.layer_scale_1 - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.layer_scale_2 - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.proj_1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.proj_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv0.weight - torch.Size([64, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv0_1.weight - torch.Size([64, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv0_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv0_2.weight - torch.Size([64, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv0_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv1_1.weight - torch.Size([64, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv1_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv1_2.weight - torch.Size([64, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv1_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv2_1.weight - torch.Size([64, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv2_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv2_2.weight - torch.Size([64, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv2_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv3.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.spatial_gating_unit.conv3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.proj_2.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.attn.proj_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.norm2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.norm2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.mlp.fc1.weight - torch.Size([512, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.mlp.fc1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.mlp.dwconv.dwconv.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.mlp.dwconv.dwconv.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.mlp.fc2.weight - torch.Size([64, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.0.mlp.fc2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.layer_scale_1 - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.layer_scale_2 - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.proj_1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.proj_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv0.weight - torch.Size([64, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv0_1.weight - torch.Size([64, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv0_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv0_2.weight - torch.Size([64, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv0_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv1_1.weight - torch.Size([64, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv1_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv1_2.weight - torch.Size([64, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv1_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv2_1.weight - torch.Size([64, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv2_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv2_2.weight - torch.Size([64, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv2_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv3.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.spatial_gating_unit.conv3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.proj_2.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.attn.proj_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.norm2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.norm2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.mlp.fc1.weight - torch.Size([512, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.mlp.fc1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.mlp.dwconv.dwconv.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.mlp.dwconv.dwconv.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.mlp.fc2.weight - torch.Size([64, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.1.mlp.fc2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.layer_scale_1 - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.layer_scale_2 - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.proj_1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.proj_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv0.weight - torch.Size([64, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv0_1.weight - torch.Size([64, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv0_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv0_2.weight - torch.Size([64, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv0_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv1_1.weight - torch.Size([64, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv1_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv1_2.weight - torch.Size([64, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv1_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv2_1.weight - torch.Size([64, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv2_1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv2_2.weight - torch.Size([64, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv2_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv3.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.spatial_gating_unit.conv3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.proj_2.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.attn.proj_2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.norm2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.norm2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.mlp.fc1.weight - torch.Size([512, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.mlp.fc1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.mlp.dwconv.dwconv.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.mlp.dwconv.dwconv.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.mlp.fc2.weight - torch.Size([64, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block1.2.mlp.fc2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed2.proj.weight - torch.Size([128, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed2.proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed2.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed2.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.layer_scale_1 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.layer_scale_2 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.proj_1.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.proj_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv0_1.weight - torch.Size([128, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv0_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv0_2.weight - torch.Size([128, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv0_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv1_1.weight - torch.Size([128, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv1_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv1_2.weight - torch.Size([128, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv1_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv2_1.weight - torch.Size([128, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv2_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv2_2.weight - torch.Size([128, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv2_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv3.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.spatial_gating_unit.conv3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.proj_2.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.attn.proj_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.mlp.fc1.weight - torch.Size([1024, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.mlp.dwconv.dwconv.weight - torch.Size([1024, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.mlp.dwconv.dwconv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.mlp.fc2.weight - torch.Size([128, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.0.mlp.fc2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.layer_scale_1 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.layer_scale_2 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.proj_1.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.proj_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv0_1.weight - torch.Size([128, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv0_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv0_2.weight - torch.Size([128, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv0_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv1_1.weight - torch.Size([128, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv1_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv1_2.weight - torch.Size([128, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv1_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv2_1.weight - torch.Size([128, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv2_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv2_2.weight - torch.Size([128, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv2_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv3.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.spatial_gating_unit.conv3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.proj_2.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.attn.proj_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.mlp.fc1.weight - torch.Size([1024, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.mlp.dwconv.dwconv.weight - torch.Size([1024, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.mlp.dwconv.dwconv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.mlp.fc2.weight - torch.Size([128, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.1.mlp.fc2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.layer_scale_1 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.layer_scale_2 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.proj_1.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.proj_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv0_1.weight - torch.Size([128, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv0_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv0_2.weight - torch.Size([128, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv0_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv1_1.weight - torch.Size([128, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv1_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv1_2.weight - torch.Size([128, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv1_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv2_1.weight - torch.Size([128, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv2_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv2_2.weight - torch.Size([128, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv2_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv3.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.spatial_gating_unit.conv3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.proj_2.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.attn.proj_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.mlp.fc1.weight - torch.Size([1024, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.mlp.dwconv.dwconv.weight - torch.Size([1024, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.mlp.dwconv.dwconv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.mlp.fc2.weight - torch.Size([128, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.2.mlp.fc2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.layer_scale_1 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.layer_scale_2 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.proj_1.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.proj_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv0_1.weight - torch.Size([128, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv0_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv0_2.weight - torch.Size([128, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv0_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv1_1.weight - torch.Size([128, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv1_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv1_2.weight - torch.Size([128, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv1_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv2_1.weight - torch.Size([128, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv2_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv2_2.weight - torch.Size([128, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv2_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv3.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.spatial_gating_unit.conv3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.proj_2.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.attn.proj_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.mlp.fc1.weight - torch.Size([1024, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.mlp.dwconv.dwconv.weight - torch.Size([1024, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.mlp.dwconv.dwconv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.mlp.fc2.weight - torch.Size([128, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.3.mlp.fc2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.layer_scale_1 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.layer_scale_2 - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.proj_1.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.proj_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv0.weight - torch.Size([128, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv0_1.weight - torch.Size([128, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv0_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv0_2.weight - torch.Size([128, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv0_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv1_1.weight - torch.Size([128, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv1_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv1_2.weight - torch.Size([128, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv1_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv2_1.weight - torch.Size([128, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv2_1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv2_2.weight - torch.Size([128, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv2_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv3.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.spatial_gating_unit.conv3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.proj_2.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.attn.proj_2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.mlp.fc1.weight - torch.Size([1024, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.mlp.dwconv.dwconv.weight - torch.Size([1024, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.mlp.dwconv.dwconv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.mlp.fc2.weight - torch.Size([128, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block2.4.mlp.fc2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed3.proj.weight - torch.Size([320, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed3.proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed3.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed3.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.0.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.1.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.2.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.3.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.4.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.5.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.6.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.7.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.8.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.9.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.10.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.11.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.12.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.13.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.14.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.15.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.16.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.17.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.18.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.19.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.20.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.21.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.22.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.23.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.24.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.25.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.layer_scale_1 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.layer_scale_2 - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.proj_1.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.proj_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv0.weight - torch.Size([320, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv0_1.weight - torch.Size([320, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv0_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv0_2.weight - torch.Size([320, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv0_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv1_1.weight - torch.Size([320, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv1_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv1_2.weight - torch.Size([320, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv1_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv2_1.weight - torch.Size([320, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv2_1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv2_2.weight - torch.Size([320, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv2_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv3.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.spatial_gating_unit.conv3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.proj_2.weight - torch.Size([320, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.attn.proj_2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.mlp.fc1.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.mlp.fc1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.mlp.fc2.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block3.26.mlp.fc2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed4.proj.weight - torch.Size([512, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed4.proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed4.norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.patch_embed4.norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.layer_scale_1 - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.layer_scale_2 - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.proj_1.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.proj_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv0.weight - torch.Size([512, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv0_1.weight - torch.Size([512, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv0_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv0_2.weight - torch.Size([512, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv0_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv1_1.weight - torch.Size([512, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv1_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv1_2.weight - torch.Size([512, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv1_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv2_1.weight - torch.Size([512, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv2_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv2_2.weight - torch.Size([512, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv2_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv3.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.spatial_gating_unit.conv3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.proj_2.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.attn.proj_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.mlp.fc1.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.mlp.fc1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.mlp.dwconv.dwconv.weight - torch.Size([2048, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.mlp.dwconv.dwconv.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.mlp.fc2.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.0.mlp.fc2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.layer_scale_1 - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.layer_scale_2 - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.proj_1.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.proj_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv0.weight - torch.Size([512, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv0_1.weight - torch.Size([512, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv0_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv0_2.weight - torch.Size([512, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv0_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv1_1.weight - torch.Size([512, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv1_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv1_2.weight - torch.Size([512, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv1_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv2_1.weight - torch.Size([512, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv2_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv2_2.weight - torch.Size([512, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv2_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv3.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.spatial_gating_unit.conv3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.proj_2.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.attn.proj_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.mlp.fc1.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.mlp.fc1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.mlp.dwconv.dwconv.weight - torch.Size([2048, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.mlp.dwconv.dwconv.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.mlp.fc2.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.1.mlp.fc2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.layer_scale_1 - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.layer_scale_2 - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.proj_1.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.proj_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv0.weight - torch.Size([512, 1, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv0_1.weight - torch.Size([512, 1, 1, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv0_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv0_2.weight - torch.Size([512, 1, 7, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv0_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv1_1.weight - torch.Size([512, 1, 1, 11]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv1_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv1_2.weight - torch.Size([512, 1, 11, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv1_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv2_1.weight - torch.Size([512, 1, 1, 21]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv2_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv2_2.weight - torch.Size([512, 1, 21, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv2_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv3.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.spatial_gating_unit.conv3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.proj_2.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.attn.proj_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.mlp.fc1.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.mlp.fc1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.mlp.dwconv.dwconv.weight - torch.Size([2048, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.mlp.dwconv.dwconv.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.mlp.fc2.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.block4.2.mlp.fc2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm4.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_seg.weight - torch.Size([5, 1024, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([5]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.squeeze.conv.weight - torch.Size([1024, 960, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.squeeze.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.squeeze.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.hamburger.ham_in.conv.weight - torch.Size([1024, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.hamburger.ham_in.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.hamburger.ham_out.conv.weight - torch.Size([1024, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.hamburger.ham_out.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.hamburger.ham_out.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.align.conv.weight - torch.Size([1024, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.align.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.align.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2022-11-29 16:29:45,530 - mmseg - INFO - EncoderDecoder(
  (backbone): MSCAN(
    (patch_embed1): StemConv(
      (proj): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): _BatchNormXd(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): GELU()
        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)
            (conv0_1): Conv2d(64, 64, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=64)
            (conv0_2): Conv2d(64, 64, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=64)
            (conv1_1): Conv2d(64, 64, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=64)
            (conv1_2): Conv2d(64, 64, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=64)
            (conv2_1): Conv2d(64, 64, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=64)
            (conv2_2): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): Identity()
        (norm2): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)
            (conv0_1): Conv2d(64, 64, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=64)
            (conv0_2): Conv2d(64, 64, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=64)
            (conv1_1): Conv2d(64, 64, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=64)
            (conv1_2): Conv2d(64, 64, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=64)
            (conv2_1): Conv2d(64, 64, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=64)
            (conv2_2): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)
            (conv0_1): Conv2d(64, 64, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=64)
            (conv0_2): Conv2d(64, 64, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=64)
            (conv1_1): Conv2d(64, 64, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=64)
            (conv1_2): Conv2d(64, 64, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=64)
            (conv2_1): Conv2d(64, 64, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=64)
            (conv2_2): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (patch_embed2): OverlapPatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (block2): ModuleList(
      (0): Block(
        (norm1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)
            (conv0_1): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=128)
            (conv0_2): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=128)
            (conv1_1): Conv2d(128, 128, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=128)
            (conv1_2): Conv2d(128, 128, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=128)
            (conv2_1): Conv2d(128, 128, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=128)
            (conv2_2): Conv2d(128, 128, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=128)
            (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
          )
          (act): GELU()
          (fc2): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)
            (conv0_1): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=128)
            (conv0_2): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=128)
            (conv1_1): Conv2d(128, 128, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=128)
            (conv1_2): Conv2d(128, 128, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=128)
            (conv2_1): Conv2d(128, 128, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=128)
            (conv2_2): Conv2d(128, 128, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=128)
            (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
          )
          (act): GELU()
          (fc2): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)
            (conv0_1): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=128)
            (conv0_2): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=128)
            (conv1_1): Conv2d(128, 128, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=128)
            (conv1_2): Conv2d(128, 128, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=128)
            (conv2_1): Conv2d(128, 128, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=128)
            (conv2_2): Conv2d(128, 128, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=128)
            (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
          )
          (act): GELU()
          (fc2): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)
            (conv0_1): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=128)
            (conv0_2): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=128)
            (conv1_1): Conv2d(128, 128, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=128)
            (conv1_2): Conv2d(128, 128, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=128)
            (conv2_1): Conv2d(128, 128, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=128)
            (conv2_2): Conv2d(128, 128, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=128)
            (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
          )
          (act): GELU()
          (fc2): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)
            (conv0_1): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=128)
            (conv0_2): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=128)
            (conv1_1): Conv2d(128, 128, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=128)
            (conv1_2): Conv2d(128, 128, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=128)
            (conv2_1): Conv2d(128, 128, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=128)
            (conv2_2): Conv2d(128, 128, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=128)
            (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
          )
          (act): GELU()
          (fc2): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (patch_embed3): OverlapPatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (block3): ModuleList(
      (0): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (24): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (25): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (26): Block(
        (norm1): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
            (conv0_1): Conv2d(320, 320, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=320)
            (conv0_2): Conv2d(320, 320, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=320)
            (conv1_1): Conv2d(320, 320, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=320)
            (conv1_2): Conv2d(320, 320, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=320)
            (conv2_1): Conv2d(320, 320, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=320)
            (conv2_2): Conv2d(320, 320, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=320)
            (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (patch_embed4): OverlapPatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (block4): ModuleList(
      (0): Block(
        (norm1): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
            (conv0_1): Conv2d(512, 512, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=512)
            (conv0_2): Conv2d(512, 512, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=512)
            (conv1_1): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)
            (conv1_2): Conv2d(512, 512, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=512)
            (conv2_1): Conv2d(512, 512, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=512)
            (conv2_2): Conv2d(512, 512, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=512)
            (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU()
          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
            (conv0_1): Conv2d(512, 512, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=512)
            (conv0_2): Conv2d(512, 512, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=512)
            (conv1_1): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)
            (conv1_2): Conv2d(512, 512, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=512)
            (conv2_1): Conv2d(512, 512, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=512)
            (conv2_2): Conv2d(512, 512, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=512)
            (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU()
          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (attn): SpatialAttention(
          (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (activation): GELU()
          (spatial_gating_unit): AttentionModule(
            (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
            (conv0_1): Conv2d(512, 512, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=512)
            (conv0_2): Conv2d(512, 512, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=512)
            (conv1_1): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)
            (conv1_2): Conv2d(512, 512, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=512)
            (conv2_1): Conv2d(512, 512, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=512)
            (conv2_2): Conv2d(512, 512, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=512)
            (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (drop_path): DropPath()
        (norm2): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (mlp): Mlp(
          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU()
          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decode_head): LightHamHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): Conv2d(1024, 5, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (squeeze): ConvModule(
      (conv): Conv2d(960, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
      (activate): ReLU(inplace=True)
    )
    (hamburger): Hamburger(
      (ham_in): ConvModule(
        (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (ham): NMF2D()
      (ham_out): ConvModule(
        (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
      )
    )
    (align): ConvModule(
      (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
      (activate): ReLU(inplace=True)
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2022-11-29 16:29:45,552 - mmseg - INFO - Loaded 933 images
2022-11-29 16:29:48,732 - mmseg - INFO - Loaded 1602 images
2022-11-29 16:29:48,732 - mmseg - INFO - Start running, host: yl407@dcc-carlsonlab-gpu-13, work_dir: /work/yl407/SegNeXt/work_dirs/segnext.large.512x512.landcover.40k_weight_sub_nopre
2022-11-29 16:29:48,732 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-11-29 16:29:48,733 - mmseg - INFO - workflow: [('train', 1)], max: 40000 iters
2022-11-29 16:29:48,733 - mmseg - INFO - Checkpoints will be saved to /work/yl407/SegNeXt/work_dirs/segnext.large.512x512.landcover.40k_weight_sub_nopre by HardDiskBackend.
2022-11-29 16:30:04,979 - mmseg - INFO - Iter [50/40000]	lr: 1.958e-06, eta: 3:33:41, time: 0.321, data_time: 0.008, memory: 5537, decode.loss_ce: 0.8064, decode.acc_seg: 53.4744, loss: 0.8064
2022-11-29 16:30:19,327 - mmseg - INFO - Iter [100/40000]	lr: 3.950e-06, eta: 3:22:07, time: 0.287, data_time: 0.006, memory: 5537, decode.loss_ce: 0.7175, decode.acc_seg: 43.7047, loss: 0.7175
2022-11-29 16:30:34,966 - mmseg - INFO - Iter [150/40000]	lr: 5.938e-06, eta: 3:23:48, time: 0.313, data_time: 0.006, memory: 5537, decode.loss_ce: 0.7505, decode.acc_seg: 46.1556, loss: 0.7505
2022-11-29 16:30:50,254 - mmseg - INFO - Iter [200/40000]	lr: 7.920e-06, eta: 3:23:22, time: 0.306, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6904, decode.acc_seg: 55.8088, loss: 0.6904
2022-11-29 16:31:03,745 - mmseg - INFO - Iter [250/40000]	lr: 9.898e-06, eta: 3:18:14, time: 0.270, data_time: 0.006, memory: 5537, decode.loss_ce: 0.7338, decode.acc_seg: 48.4349, loss: 0.7338
2022-11-29 16:31:18,060 - mmseg - INFO - Iter [300/40000]	lr: 1.187e-05, eta: 3:16:33, time: 0.286, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6050, decode.acc_seg: 53.5333, loss: 0.6050
2022-11-29 16:31:34,438 - mmseg - INFO - Iter [350/40000]	lr: 1.384e-05, eta: 3:19:11, time: 0.328, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6032, decode.acc_seg: 52.2948, loss: 0.6032
2022-11-29 16:31:50,885 - mmseg - INFO - Iter [400/40000]	lr: 1.580e-05, eta: 3:21:12, time: 0.329, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5856, decode.acc_seg: 59.1028, loss: 0.5856
2022-11-29 16:32:04,498 - mmseg - INFO - Iter [450/40000]	lr: 1.776e-05, eta: 3:18:33, time: 0.272, data_time: 0.006, memory: 5537, decode.loss_ce: 0.8295, decode.acc_seg: 58.9165, loss: 0.8295
2022-11-29 16:32:18,043 - mmseg - INFO - Iter [500/40000]	lr: 1.971e-05, eta: 3:16:18, time: 0.271, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6709, decode.acc_seg: 57.6875, loss: 0.6709
2022-11-29 16:32:31,660 - mmseg - INFO - Iter [550/40000]	lr: 2.166e-05, eta: 3:14:30, time: 0.272, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6199, decode.acc_seg: 61.5888, loss: 0.6199
2022-11-29 16:32:45,174 - mmseg - INFO - Iter [600/40000]	lr: 2.360e-05, eta: 3:12:52, time: 0.270, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6356, decode.acc_seg: 53.3668, loss: 0.6356
2022-11-29 16:32:58,693 - mmseg - INFO - Iter [650/40000]	lr: 2.554e-05, eta: 3:11:26, time: 0.270, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5185, decode.acc_seg: 62.4521, loss: 0.5185
2022-11-29 16:33:12,227 - mmseg - INFO - Iter [700/40000]	lr: 2.747e-05, eta: 3:10:12, time: 0.271, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6421, decode.acc_seg: 59.7583, loss: 0.6421
2022-11-29 16:33:27,767 - mmseg - INFO - Iter [750/40000]	lr: 2.940e-05, eta: 3:10:51, time: 0.311, data_time: 0.007, memory: 5537, decode.loss_ce: 0.6024, decode.acc_seg: 50.5173, loss: 0.6024
2022-11-29 16:33:43,689 - mmseg - INFO - Iter [800/40000]	lr: 3.132e-05, eta: 3:11:42, time: 0.318, data_time: 0.007, memory: 5537, decode.loss_ce: 0.6145, decode.acc_seg: 55.1917, loss: 0.6145
2022-11-29 16:33:57,465 - mmseg - INFO - Iter [850/40000]	lr: 3.324e-05, eta: 3:10:46, time: 0.276, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5394, decode.acc_seg: 61.7826, loss: 0.5394
2022-11-29 16:34:11,069 - mmseg - INFO - Iter [900/40000]	lr: 3.515e-05, eta: 3:09:47, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.6584, decode.acc_seg: 58.7096, loss: 0.6584
2022-11-29 16:34:26,815 - mmseg - INFO - Iter [950/40000]	lr: 3.706e-05, eta: 3:10:21, time: 0.315, data_time: 0.049, memory: 5537, decode.loss_ce: 0.5161, decode.acc_seg: 66.0054, loss: 0.5161
2022-11-29 16:34:40,456 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 16:34:40,457 - mmseg - INFO - Iter [1000/40000]	lr: 3.896e-05, eta: 3:09:28, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.6165, decode.acc_seg: 65.5822, loss: 0.6165
2022-11-29 16:34:54,125 - mmseg - INFO - Iter [1050/40000]	lr: 4.086e-05, eta: 3:08:40, time: 0.273, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4061, decode.acc_seg: 67.3551, loss: 0.4061
2022-11-29 16:35:07,698 - mmseg - INFO - Iter [1100/40000]	lr: 4.275e-05, eta: 3:07:51, time: 0.271, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5202, decode.acc_seg: 65.1018, loss: 0.5202
2022-11-29 16:35:21,272 - mmseg - INFO - Iter [1150/40000]	lr: 4.464e-05, eta: 3:07:06, time: 0.271, data_time: 0.006, memory: 5537, decode.loss_ce: 0.7730, decode.acc_seg: 52.9786, loss: 0.7730
2022-11-29 16:35:34,847 - mmseg - INFO - Iter [1200/40000]	lr: 4.652e-05, eta: 3:06:23, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5787, decode.acc_seg: 65.6970, loss: 0.5787
2022-11-29 16:35:48,493 - mmseg - INFO - Iter [1250/40000]	lr: 4.840e-05, eta: 3:05:45, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5064, decode.acc_seg: 61.6340, loss: 0.5064
2022-11-29 16:36:02,185 - mmseg - INFO - Iter [1300/40000]	lr: 5.027e-05, eta: 3:05:10, time: 0.274, data_time: 0.007, memory: 5537, decode.loss_ce: 0.6233, decode.acc_seg: 60.8281, loss: 0.6233
2022-11-29 16:36:15,842 - mmseg - INFO - Iter [1350/40000]	lr: 5.214e-05, eta: 3:04:36, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4912, decode.acc_seg: 69.7009, loss: 0.4912
2022-11-29 16:36:29,462 - mmseg - INFO - Iter [1400/40000]	lr: 5.400e-05, eta: 3:04:02, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.7246, decode.acc_seg: 61.4329, loss: 0.7246
2022-11-29 16:36:43,172 - mmseg - INFO - Iter [1450/40000]	lr: 5.586e-05, eta: 3:03:32, time: 0.274, data_time: 0.007, memory: 5537, decode.loss_ce: 0.6964, decode.acc_seg: 49.2091, loss: 0.6964
2022-11-29 16:36:56,747 - mmseg - INFO - Iter [1500/40000]	lr: 5.771e-05, eta: 3:02:59, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5197, decode.acc_seg: 65.8823, loss: 0.5197
2022-11-29 16:37:10,342 - mmseg - INFO - Iter [1550/40000]	lr: 5.768e-05, eta: 3:02:29, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.6049, decode.acc_seg: 60.3432, loss: 0.6049
2022-11-29 16:37:23,884 - mmseg - INFO - Iter [1600/40000]	lr: 5.760e-05, eta: 3:01:58, time: 0.271, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6371, decode.acc_seg: 56.3138, loss: 0.6371
2022-11-29 16:37:37,323 - mmseg - INFO - Iter [1650/40000]	lr: 5.753e-05, eta: 3:01:25, time: 0.269, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6403, decode.acc_seg: 58.5908, loss: 0.6403
2022-11-29 16:37:50,856 - mmseg - INFO - Iter [1700/40000]	lr: 5.745e-05, eta: 3:00:56, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5865, decode.acc_seg: 63.8629, loss: 0.5865
2022-11-29 16:38:06,984 - mmseg - INFO - Iter [1750/40000]	lr: 5.738e-05, eta: 3:01:25, time: 0.323, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5659, decode.acc_seg: 66.3715, loss: 0.5659
2022-11-29 16:38:22,974 - mmseg - INFO - Iter [1800/40000]	lr: 5.730e-05, eta: 3:01:48, time: 0.320, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5608, decode.acc_seg: 66.5125, loss: 0.5608
2022-11-29 16:38:36,850 - mmseg - INFO - Iter [1850/40000]	lr: 5.723e-05, eta: 3:01:25, time: 0.277, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4746, decode.acc_seg: 71.6633, loss: 0.4746
2022-11-29 16:38:52,401 - mmseg - INFO - Iter [1900/40000]	lr: 5.715e-05, eta: 3:01:37, time: 0.311, data_time: 0.048, memory: 5537, decode.loss_ce: 0.5441, decode.acc_seg: 68.9736, loss: 0.5441
2022-11-29 16:39:05,672 - mmseg - INFO - Iter [1950/40000]	lr: 5.708e-05, eta: 3:01:02, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4802, decode.acc_seg: 71.6789, loss: 0.4802
2022-11-29 16:39:18,998 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 16:39:18,999 - mmseg - INFO - Iter [2000/40000]	lr: 5.700e-05, eta: 3:00:30, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4885, decode.acc_seg: 64.8809, loss: 0.4885
2022-11-29 16:39:32,265 - mmseg - INFO - Iter [2050/40000]	lr: 5.693e-05, eta: 2:59:58, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4494, decode.acc_seg: 68.5852, loss: 0.4494
2022-11-29 16:39:45,534 - mmseg - INFO - Iter [2100/40000]	lr: 5.685e-05, eta: 2:59:26, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5219, decode.acc_seg: 69.1999, loss: 0.5219
2022-11-29 16:39:58,896 - mmseg - INFO - Iter [2150/40000]	lr: 5.678e-05, eta: 2:58:57, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4875, decode.acc_seg: 67.5601, loss: 0.4875
2022-11-29 16:40:12,246 - mmseg - INFO - Iter [2200/40000]	lr: 5.670e-05, eta: 2:58:29, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6484, decode.acc_seg: 61.6753, loss: 0.6484
2022-11-29 16:40:25,699 - mmseg - INFO - Iter [2250/40000]	lr: 5.663e-05, eta: 2:58:02, time: 0.269, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5908, decode.acc_seg: 60.3118, loss: 0.5908
2022-11-29 16:40:39,013 - mmseg - INFO - Iter [2300/40000]	lr: 5.655e-05, eta: 2:57:35, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6716, decode.acc_seg: 60.4807, loss: 0.6716
2022-11-29 16:40:52,281 - mmseg - INFO - Iter [2350/40000]	lr: 5.648e-05, eta: 2:57:07, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5119, decode.acc_seg: 66.0895, loss: 0.5119
2022-11-29 16:41:05,649 - mmseg - INFO - Iter [2400/40000]	lr: 5.640e-05, eta: 2:56:41, time: 0.267, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4474, decode.acc_seg: 75.0957, loss: 0.4474
2022-11-29 16:41:18,910 - mmseg - INFO - Iter [2450/40000]	lr: 5.633e-05, eta: 2:56:14, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4522, decode.acc_seg: 74.3760, loss: 0.4522
2022-11-29 16:41:32,243 - mmseg - INFO - Iter [2500/40000]	lr: 5.625e-05, eta: 2:55:49, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5065, decode.acc_seg: 70.5958, loss: 0.5065
2022-11-29 16:41:45,535 - mmseg - INFO - Iter [2550/40000]	lr: 5.618e-05, eta: 2:55:23, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.6020, decode.acc_seg: 67.7853, loss: 0.6020
2022-11-29 16:41:58,780 - mmseg - INFO - Iter [2600/40000]	lr: 5.610e-05, eta: 2:54:57, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5482, decode.acc_seg: 72.6407, loss: 0.5482
2022-11-29 16:42:12,090 - mmseg - INFO - Iter [2650/40000]	lr: 5.603e-05, eta: 2:54:33, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5145, decode.acc_seg: 65.2846, loss: 0.5145
2022-11-29 16:42:25,405 - mmseg - INFO - Iter [2700/40000]	lr: 5.595e-05, eta: 2:54:09, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4494, decode.acc_seg: 72.4184, loss: 0.4494
2022-11-29 16:42:38,718 - mmseg - INFO - Iter [2750/40000]	lr: 5.588e-05, eta: 2:53:46, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5842, decode.acc_seg: 68.0020, loss: 0.5842
2022-11-29 16:42:54,160 - mmseg - INFO - Iter [2800/40000]	lr: 5.580e-05, eta: 2:53:51, time: 0.309, data_time: 0.048, memory: 5537, decode.loss_ce: 0.4901, decode.acc_seg: 63.8534, loss: 0.4901
2022-11-29 16:43:08,116 - mmseg - INFO - Iter [2850/40000]	lr: 5.573e-05, eta: 2:53:36, time: 0.279, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4446, decode.acc_seg: 65.9324, loss: 0.4446
2022-11-29 16:43:21,366 - mmseg - INFO - Iter [2900/40000]	lr: 5.565e-05, eta: 2:53:12, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4456, decode.acc_seg: 79.0242, loss: 0.4456
2022-11-29 16:43:34,933 - mmseg - INFO - Iter [2950/40000]	lr: 5.558e-05, eta: 2:52:53, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5872, decode.acc_seg: 69.9733, loss: 0.5872
2022-11-29 16:43:50,249 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 16:43:50,249 - mmseg - INFO - Iter [3000/40000]	lr: 5.550e-05, eta: 2:52:55, time: 0.306, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4049, decode.acc_seg: 79.6740, loss: 0.4049
2022-11-29 16:44:03,504 - mmseg - INFO - Iter [3050/40000]	lr: 5.543e-05, eta: 2:52:32, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4145, decode.acc_seg: 76.3649, loss: 0.4145
2022-11-29 16:44:16,804 - mmseg - INFO - Iter [3100/40000]	lr: 5.535e-05, eta: 2:52:09, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5745, decode.acc_seg: 59.9455, loss: 0.5745
2022-11-29 16:44:30,155 - mmseg - INFO - Iter [3150/40000]	lr: 5.528e-05, eta: 2:51:48, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3889, decode.acc_seg: 75.4276, loss: 0.3889
2022-11-29 16:44:43,430 - mmseg - INFO - Iter [3200/40000]	lr: 5.520e-05, eta: 2:51:26, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3966, decode.acc_seg: 74.5214, loss: 0.3966
2022-11-29 16:44:56,718 - mmseg - INFO - Iter [3250/40000]	lr: 5.513e-05, eta: 2:51:04, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4496, decode.acc_seg: 71.2223, loss: 0.4496
2022-11-29 16:45:11,351 - mmseg - INFO - Iter [3300/40000]	lr: 5.505e-05, eta: 2:50:57, time: 0.293, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3403, decode.acc_seg: 82.1190, loss: 0.3403
2022-11-29 16:45:25,845 - mmseg - INFO - Iter [3350/40000]	lr: 5.498e-05, eta: 2:50:49, time: 0.290, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4965, decode.acc_seg: 68.1712, loss: 0.4965
2022-11-29 16:45:39,130 - mmseg - INFO - Iter [3400/40000]	lr: 5.490e-05, eta: 2:50:28, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4677, decode.acc_seg: 74.6665, loss: 0.4677
2022-11-29 16:45:52,404 - mmseg - INFO - Iter [3450/40000]	lr: 5.483e-05, eta: 2:50:06, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4909, decode.acc_seg: 62.8835, loss: 0.4909
2022-11-29 16:46:05,698 - mmseg - INFO - Iter [3500/40000]	lr: 5.475e-05, eta: 2:49:45, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5216, decode.acc_seg: 57.9965, loss: 0.5216
2022-11-29 16:46:18,970 - mmseg - INFO - Iter [3550/40000]	lr: 5.468e-05, eta: 2:49:24, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5493, decode.acc_seg: 66.3209, loss: 0.5493
2022-11-29 16:46:32,223 - mmseg - INFO - Iter [3600/40000]	lr: 5.460e-05, eta: 2:49:03, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4253, decode.acc_seg: 72.9927, loss: 0.4253
2022-11-29 16:46:45,564 - mmseg - INFO - Iter [3650/40000]	lr: 5.453e-05, eta: 2:48:44, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5478, decode.acc_seg: 73.1539, loss: 0.5478
2022-11-29 16:46:58,836 - mmseg - INFO - Iter [3700/40000]	lr: 5.445e-05, eta: 2:48:23, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4993, decode.acc_seg: 66.6480, loss: 0.4993
2022-11-29 16:47:14,555 - mmseg - INFO - Iter [3750/40000]	lr: 5.438e-05, eta: 2:48:27, time: 0.314, data_time: 0.048, memory: 5537, decode.loss_ce: 0.5241, decode.acc_seg: 64.7090, loss: 0.5241
2022-11-29 16:47:28,234 - mmseg - INFO - Iter [3800/40000]	lr: 5.430e-05, eta: 2:48:10, time: 0.274, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4141, decode.acc_seg: 73.7338, loss: 0.4141
2022-11-29 16:47:42,596 - mmseg - INFO - Iter [3850/40000]	lr: 5.423e-05, eta: 2:48:00, time: 0.287, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5028, decode.acc_seg: 70.5755, loss: 0.5028
2022-11-29 16:47:56,011 - mmseg - INFO - Iter [3900/40000]	lr: 5.415e-05, eta: 2:47:41, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3740, decode.acc_seg: 75.3441, loss: 0.3740
2022-11-29 16:48:09,293 - mmseg - INFO - Iter [3950/40000]	lr: 5.408e-05, eta: 2:47:22, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4219, decode.acc_seg: 78.2259, loss: 0.4219
2022-11-29 16:48:22,603 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 16:48:22,603 - mmseg - INFO - Iter [4000/40000]	lr: 5.400e-05, eta: 2:47:02, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4375, decode.acc_seg: 71.6264, loss: 0.4375
2022-11-29 16:48:36,237 - mmseg - INFO - Iter [4050/40000]	lr: 5.393e-05, eta: 2:46:46, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4601, decode.acc_seg: 70.5326, loss: 0.4601
2022-11-29 16:48:49,625 - mmseg - INFO - Iter [4100/40000]	lr: 5.385e-05, eta: 2:46:27, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4725, decode.acc_seg: 72.6731, loss: 0.4725
2022-11-29 16:49:02,902 - mmseg - INFO - Iter [4150/40000]	lr: 5.378e-05, eta: 2:46:08, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4191, decode.acc_seg: 71.8964, loss: 0.4191
2022-11-29 16:49:16,190 - mmseg - INFO - Iter [4200/40000]	lr: 5.370e-05, eta: 2:45:48, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4077, decode.acc_seg: 78.8767, loss: 0.4077
2022-11-29 16:49:29,494 - mmseg - INFO - Iter [4250/40000]	lr: 5.363e-05, eta: 2:45:30, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5735, decode.acc_seg: 68.1121, loss: 0.5735
2022-11-29 16:49:42,849 - mmseg - INFO - Iter [4300/40000]	lr: 5.355e-05, eta: 2:45:11, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4705, decode.acc_seg: 69.3499, loss: 0.4705
2022-11-29 16:49:56,146 - mmseg - INFO - Iter [4350/40000]	lr: 5.348e-05, eta: 2:44:53, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5066, decode.acc_seg: 73.6867, loss: 0.5066
2022-11-29 16:50:09,369 - mmseg - INFO - Iter [4400/40000]	lr: 5.340e-05, eta: 2:44:33, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3854, decode.acc_seg: 75.1406, loss: 0.3854
2022-11-29 16:50:22,661 - mmseg - INFO - Iter [4450/40000]	lr: 5.333e-05, eta: 2:44:15, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4207, decode.acc_seg: 76.0824, loss: 0.4207
2022-11-29 16:50:35,975 - mmseg - INFO - Iter [4500/40000]	lr: 5.325e-05, eta: 2:43:57, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3800, decode.acc_seg: 77.6351, loss: 0.3800
2022-11-29 16:50:49,405 - mmseg - INFO - Iter [4550/40000]	lr: 5.318e-05, eta: 2:43:40, time: 0.269, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5172, decode.acc_seg: 69.0772, loss: 0.5172
2022-11-29 16:51:02,780 - mmseg - INFO - Iter [4600/40000]	lr: 5.310e-05, eta: 2:43:22, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5077, decode.acc_seg: 67.9116, loss: 0.5077
2022-11-29 16:51:16,117 - mmseg - INFO - Iter [4650/40000]	lr: 5.303e-05, eta: 2:43:04, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4757, decode.acc_seg: 68.1375, loss: 0.4757
2022-11-29 16:51:31,799 - mmseg - INFO - Iter [4700/40000]	lr: 5.295e-05, eta: 2:43:04, time: 0.314, data_time: 0.048, memory: 5537, decode.loss_ce: 0.6295, decode.acc_seg: 59.3850, loss: 0.6295
2022-11-29 16:51:45,147 - mmseg - INFO - Iter [4750/40000]	lr: 5.288e-05, eta: 2:42:47, time: 0.267, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4805, decode.acc_seg: 66.6007, loss: 0.4805
2022-11-29 16:51:58,461 - mmseg - INFO - Iter [4800/40000]	lr: 5.280e-05, eta: 2:42:29, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4693, decode.acc_seg: 70.9842, loss: 0.4693
2022-11-29 16:52:11,782 - mmseg - INFO - Iter [4850/40000]	lr: 5.273e-05, eta: 2:42:11, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3753, decode.acc_seg: 81.0424, loss: 0.3753
2022-11-29 16:52:25,109 - mmseg - INFO - Iter [4900/40000]	lr: 5.265e-05, eta: 2:41:54, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.5731, decode.acc_seg: 70.7831, loss: 0.5731
2022-11-29 16:52:38,358 - mmseg - INFO - Iter [4950/40000]	lr: 5.258e-05, eta: 2:41:36, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4244, decode.acc_seg: 72.4430, loss: 0.4244
2022-11-29 16:52:51,726 - mmseg - INFO - Saving checkpoint at 5000 iterations
2022-11-29 16:52:54,373 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 16:52:54,373 - mmseg - INFO - Iter [5000/40000]	lr: 5.250e-05, eta: 2:41:37, time: 0.320, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4256, decode.acc_seg: 71.7274, loss: 0.4256
2022-11-29 16:54:21,083 - mmseg - INFO - per class results:
2022-11-29 16:54:21,084 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
| background | 65.21 | 96.83 |
|  building  |  1.36 |  1.41 |
|  woodland  | 30.57 | 32.31 |
|   water    |  1.54 |  1.55 |
|    road    | 11.89 | 14.88 |
+------------+-------+-------+
2022-11-29 16:54:21,084 - mmseg - INFO - Summary:
2022-11-29 16:54:21,084 - mmseg - INFO - 
+-------+-------+------+
|  aAcc |  mIoU | mAcc |
+-------+-------+------+
| 68.66 | 22.11 | 29.4 |
+-------+-------+------+
2022-11-29 16:54:21,087 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 16:54:21,088 - mmseg - INFO - Iter(val) [1602]	aAcc: 0.6866, mIoU: 0.2211, mAcc: 0.2940, IoU.background: 0.6521, IoU.building: 0.0136, IoU.woodland: 0.3057, IoU.water: 0.0154, IoU.road: 0.1189, Acc.background: 0.9683, Acc.building: 0.0141, Acc.woodland: 0.3231, Acc.water: 0.0155, Acc.road: 0.1488
2022-11-29 16:54:34,737 - mmseg - INFO - Iter [5050/40000]	lr: 5.243e-05, eta: 2:51:22, time: 2.007, data_time: 1.741, memory: 5537, decode.loss_ce: 0.5629, decode.acc_seg: 68.1667, loss: 0.5629
2022-11-29 16:54:48,461 - mmseg - INFO - Iter [5100/40000]	lr: 5.235e-05, eta: 2:51:01, time: 0.274, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5562, decode.acc_seg: 63.0388, loss: 0.5562
2022-11-29 16:55:01,991 - mmseg - INFO - Iter [5150/40000]	lr: 5.228e-05, eta: 2:50:38, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4210, decode.acc_seg: 75.8584, loss: 0.4210
2022-11-29 16:55:15,485 - mmseg - INFO - Iter [5200/40000]	lr: 5.220e-05, eta: 2:50:15, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5426, decode.acc_seg: 64.3129, loss: 0.5426
2022-11-29 16:55:29,048 - mmseg - INFO - Iter [5250/40000]	lr: 5.213e-05, eta: 2:49:53, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4393, decode.acc_seg: 70.4805, loss: 0.4393
2022-11-29 16:55:42,633 - mmseg - INFO - Iter [5300/40000]	lr: 5.205e-05, eta: 2:49:31, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4656, decode.acc_seg: 72.5637, loss: 0.4656
2022-11-29 16:55:56,204 - mmseg - INFO - Iter [5350/40000]	lr: 5.198e-05, eta: 2:49:10, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4672, decode.acc_seg: 69.8810, loss: 0.4672
2022-11-29 16:56:09,911 - mmseg - INFO - Iter [5400/40000]	lr: 5.190e-05, eta: 2:48:49, time: 0.274, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3704, decode.acc_seg: 76.4296, loss: 0.3704
2022-11-29 16:56:26,415 - mmseg - INFO - Iter [5450/40000]	lr: 5.183e-05, eta: 2:48:46, time: 0.330, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4101, decode.acc_seg: 76.2215, loss: 0.4101
2022-11-29 16:56:42,043 - mmseg - INFO - Iter [5500/40000]	lr: 5.175e-05, eta: 2:48:38, time: 0.313, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4035, decode.acc_seg: 76.5562, loss: 0.4035
2022-11-29 16:56:58,597 - mmseg - INFO - Iter [5550/40000]	lr: 5.168e-05, eta: 2:48:35, time: 0.331, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4637, decode.acc_seg: 73.6978, loss: 0.4637
2022-11-29 16:57:15,669 - mmseg - INFO - Iter [5600/40000]	lr: 5.160e-05, eta: 2:48:35, time: 0.341, data_time: 0.048, memory: 5537, decode.loss_ce: 0.3682, decode.acc_seg: 75.1451, loss: 0.3682
2022-11-29 16:57:30,076 - mmseg - INFO - Iter [5650/40000]	lr: 5.153e-05, eta: 2:48:18, time: 0.288, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4793, decode.acc_seg: 67.4742, loss: 0.4793
2022-11-29 16:57:43,830 - mmseg - INFO - Iter [5700/40000]	lr: 5.145e-05, eta: 2:47:58, time: 0.275, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4217, decode.acc_seg: 72.2604, loss: 0.4217
2022-11-29 16:57:57,400 - mmseg - INFO - Iter [5750/40000]	lr: 5.138e-05, eta: 2:47:36, time: 0.271, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4615, decode.acc_seg: 68.6395, loss: 0.4615
2022-11-29 16:58:10,896 - mmseg - INFO - Iter [5800/40000]	lr: 5.130e-05, eta: 2:47:15, time: 0.270, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4442, decode.acc_seg: 69.1903, loss: 0.4442
2022-11-29 16:58:24,563 - mmseg - INFO - Iter [5850/40000]	lr: 5.123e-05, eta: 2:46:54, time: 0.273, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3981, decode.acc_seg: 73.9588, loss: 0.3981
2022-11-29 16:58:38,282 - mmseg - INFO - Iter [5900/40000]	lr: 5.115e-05, eta: 2:46:34, time: 0.274, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4160, decode.acc_seg: 72.1020, loss: 0.4160
2022-11-29 16:58:51,959 - mmseg - INFO - Iter [5950/40000]	lr: 5.108e-05, eta: 2:46:14, time: 0.274, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3875, decode.acc_seg: 75.0239, loss: 0.3875
2022-11-29 16:59:05,583 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 16:59:05,584 - mmseg - INFO - Iter [6000/40000]	lr: 5.100e-05, eta: 2:45:53, time: 0.272, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3891, decode.acc_seg: 76.3393, loss: 0.3891
2022-11-29 16:59:19,191 - mmseg - INFO - Iter [6050/40000]	lr: 5.093e-05, eta: 2:45:33, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4482, decode.acc_seg: 77.0143, loss: 0.4482
2022-11-29 16:59:33,455 - mmseg - INFO - Iter [6100/40000]	lr: 5.085e-05, eta: 2:45:16, time: 0.285, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3640, decode.acc_seg: 78.5668, loss: 0.3640
2022-11-29 16:59:48,657 - mmseg - INFO - Iter [6150/40000]	lr: 5.078e-05, eta: 2:45:05, time: 0.304, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3890, decode.acc_seg: 75.8852, loss: 0.3890
2022-11-29 17:00:02,307 - mmseg - INFO - Iter [6200/40000]	lr: 5.070e-05, eta: 2:44:45, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3268, decode.acc_seg: 78.4538, loss: 0.3268
2022-11-29 17:00:16,062 - mmseg - INFO - Iter [6250/40000]	lr: 5.063e-05, eta: 2:44:25, time: 0.275, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4402, decode.acc_seg: 71.9519, loss: 0.4402
2022-11-29 17:00:31,377 - mmseg - INFO - Iter [6300/40000]	lr: 5.055e-05, eta: 2:44:15, time: 0.306, data_time: 0.021, memory: 5537, decode.loss_ce: 0.3924, decode.acc_seg: 77.4182, loss: 0.3924
2022-11-29 17:00:44,990 - mmseg - INFO - Iter [6350/40000]	lr: 5.048e-05, eta: 2:43:55, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4461, decode.acc_seg: 74.3435, loss: 0.4461
2022-11-29 17:00:58,660 - mmseg - INFO - Iter [6400/40000]	lr: 5.040e-05, eta: 2:43:35, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5246, decode.acc_seg: 71.8150, loss: 0.5246
2022-11-29 17:01:12,282 - mmseg - INFO - Iter [6450/40000]	lr: 5.033e-05, eta: 2:43:15, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4073, decode.acc_seg: 77.9464, loss: 0.4073
2022-11-29 17:01:26,048 - mmseg - INFO - Iter [6500/40000]	lr: 5.025e-05, eta: 2:42:56, time: 0.275, data_time: 0.008, memory: 5537, decode.loss_ce: 0.4968, decode.acc_seg: 74.3208, loss: 0.4968
2022-11-29 17:01:42,635 - mmseg - INFO - Iter [6550/40000]	lr: 5.018e-05, eta: 2:42:52, time: 0.332, data_time: 0.049, memory: 5537, decode.loss_ce: 0.4279, decode.acc_seg: 71.9236, loss: 0.4279
2022-11-29 17:01:56,370 - mmseg - INFO - Iter [6600/40000]	lr: 5.010e-05, eta: 2:42:33, time: 0.275, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3873, decode.acc_seg: 74.9698, loss: 0.3873
2022-11-29 17:02:10,026 - mmseg - INFO - Iter [6650/40000]	lr: 5.003e-05, eta: 2:42:14, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4303, decode.acc_seg: 77.0050, loss: 0.4303
2022-11-29 17:02:23,564 - mmseg - INFO - Iter [6700/40000]	lr: 4.995e-05, eta: 2:41:54, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3944, decode.acc_seg: 70.1721, loss: 0.3944
2022-11-29 17:02:37,128 - mmseg - INFO - Iter [6750/40000]	lr: 4.988e-05, eta: 2:41:34, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3664, decode.acc_seg: 75.8856, loss: 0.3664
2022-11-29 17:02:50,680 - mmseg - INFO - Iter [6800/40000]	lr: 4.980e-05, eta: 2:41:15, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3300, decode.acc_seg: 79.3595, loss: 0.3300
2022-11-29 17:03:04,229 - mmseg - INFO - Iter [6850/40000]	lr: 4.973e-05, eta: 2:40:55, time: 0.271, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3527, decode.acc_seg: 79.4779, loss: 0.3527
2022-11-29 17:03:17,761 - mmseg - INFO - Iter [6900/40000]	lr: 4.965e-05, eta: 2:40:36, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4311, decode.acc_seg: 76.2894, loss: 0.4311
2022-11-29 17:03:31,279 - mmseg - INFO - Iter [6950/40000]	lr: 4.958e-05, eta: 2:40:16, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4630, decode.acc_seg: 70.7763, loss: 0.4630
2022-11-29 17:03:44,903 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:03:44,903 - mmseg - INFO - Iter [7000/40000]	lr: 4.950e-05, eta: 2:39:57, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3706, decode.acc_seg: 73.2742, loss: 0.3706
2022-11-29 17:03:58,472 - mmseg - INFO - Iter [7050/40000]	lr: 4.943e-05, eta: 2:39:38, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4762, decode.acc_seg: 72.9844, loss: 0.4762
2022-11-29 17:04:12,042 - mmseg - INFO - Iter [7100/40000]	lr: 4.935e-05, eta: 2:39:19, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4335, decode.acc_seg: 72.8504, loss: 0.4335
2022-11-29 17:04:25,630 - mmseg - INFO - Iter [7150/40000]	lr: 4.928e-05, eta: 2:39:00, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4195, decode.acc_seg: 73.4776, loss: 0.4195
2022-11-29 17:04:39,671 - mmseg - INFO - Iter [7200/40000]	lr: 4.920e-05, eta: 2:38:43, time: 0.281, data_time: 0.014, memory: 5537, decode.loss_ce: 0.4443, decode.acc_seg: 71.7693, loss: 0.4443
2022-11-29 17:04:54,906 - mmseg - INFO - Iter [7250/40000]	lr: 4.913e-05, eta: 2:38:32, time: 0.305, data_time: 0.008, memory: 5537, decode.loss_ce: 0.3830, decode.acc_seg: 76.8323, loss: 0.3830
2022-11-29 17:05:08,689 - mmseg - INFO - Iter [7300/40000]	lr: 4.905e-05, eta: 2:38:14, time: 0.276, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4342, decode.acc_seg: 65.2178, loss: 0.4342
2022-11-29 17:05:22,276 - mmseg - INFO - Iter [7350/40000]	lr: 4.898e-05, eta: 2:37:56, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3804, decode.acc_seg: 77.0617, loss: 0.3804
2022-11-29 17:05:35,994 - mmseg - INFO - Iter [7400/40000]	lr: 4.890e-05, eta: 2:37:38, time: 0.274, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2983, decode.acc_seg: 83.3339, loss: 0.2983
2022-11-29 17:05:49,564 - mmseg - INFO - Iter [7450/40000]	lr: 4.883e-05, eta: 2:37:19, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3770, decode.acc_seg: 73.6539, loss: 0.3770
2022-11-29 17:06:05,876 - mmseg - INFO - Iter [7500/40000]	lr: 4.875e-05, eta: 2:37:12, time: 0.326, data_time: 0.049, memory: 5537, decode.loss_ce: 0.4006, decode.acc_seg: 76.6443, loss: 0.4006
2022-11-29 17:06:21,505 - mmseg - INFO - Iter [7550/40000]	lr: 4.868e-05, eta: 2:37:03, time: 0.313, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3266, decode.acc_seg: 80.2927, loss: 0.3266
2022-11-29 17:06:37,602 - mmseg - INFO - Iter [7600/40000]	lr: 4.860e-05, eta: 2:36:55, time: 0.322, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3151, decode.acc_seg: 78.8616, loss: 0.3151
2022-11-29 17:06:51,251 - mmseg - INFO - Iter [7650/40000]	lr: 4.853e-05, eta: 2:36:37, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3767, decode.acc_seg: 77.9679, loss: 0.3767
2022-11-29 17:07:04,920 - mmseg - INFO - Iter [7700/40000]	lr: 4.845e-05, eta: 2:36:19, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3921, decode.acc_seg: 80.5644, loss: 0.3921
2022-11-29 17:07:18,505 - mmseg - INFO - Iter [7750/40000]	lr: 4.838e-05, eta: 2:36:00, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5040, decode.acc_seg: 66.7152, loss: 0.5040
2022-11-29 17:07:32,146 - mmseg - INFO - Iter [7800/40000]	lr: 4.830e-05, eta: 2:35:42, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4077, decode.acc_seg: 76.1427, loss: 0.4077
2022-11-29 17:07:45,781 - mmseg - INFO - Iter [7850/40000]	lr: 4.823e-05, eta: 2:35:24, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3709, decode.acc_seg: 76.6733, loss: 0.3709
2022-11-29 17:07:59,398 - mmseg - INFO - Iter [7900/40000]	lr: 4.815e-05, eta: 2:35:06, time: 0.272, data_time: 0.008, memory: 5537, decode.loss_ce: 0.4691, decode.acc_seg: 72.6081, loss: 0.4691
2022-11-29 17:08:13,085 - mmseg - INFO - Iter [7950/40000]	lr: 4.808e-05, eta: 2:34:48, time: 0.274, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4877, decode.acc_seg: 63.1591, loss: 0.4877
2022-11-29 17:08:26,782 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:08:26,782 - mmseg - INFO - Iter [8000/40000]	lr: 4.800e-05, eta: 2:34:30, time: 0.274, data_time: 0.008, memory: 5537, decode.loss_ce: 0.3877, decode.acc_seg: 75.0080, loss: 0.3877
2022-11-29 17:08:40,879 - mmseg - INFO - Iter [8050/40000]	lr: 4.793e-05, eta: 2:34:14, time: 0.282, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3838, decode.acc_seg: 75.6096, loss: 0.3838
2022-11-29 17:08:54,861 - mmseg - INFO - Iter [8100/40000]	lr: 4.785e-05, eta: 2:33:58, time: 0.280, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4328, decode.acc_seg: 71.4989, loss: 0.4328
2022-11-29 17:09:08,382 - mmseg - INFO - Iter [8150/40000]	lr: 4.778e-05, eta: 2:33:40, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3442, decode.acc_seg: 77.7370, loss: 0.3442
2022-11-29 17:09:22,108 - mmseg - INFO - Iter [8200/40000]	lr: 4.770e-05, eta: 2:33:22, time: 0.275, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3357, decode.acc_seg: 77.1829, loss: 0.3357
2022-11-29 17:09:35,690 - mmseg - INFO - Iter [8250/40000]	lr: 4.763e-05, eta: 2:33:04, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4181, decode.acc_seg: 73.7075, loss: 0.4181
2022-11-29 17:09:49,295 - mmseg - INFO - Iter [8300/40000]	lr: 4.755e-05, eta: 2:32:47, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3409, decode.acc_seg: 73.5865, loss: 0.3409
2022-11-29 17:10:02,931 - mmseg - INFO - Iter [8350/40000]	lr: 4.748e-05, eta: 2:32:29, time: 0.273, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4105, decode.acc_seg: 75.8324, loss: 0.4105
2022-11-29 17:10:18,662 - mmseg - INFO - Iter [8400/40000]	lr: 4.740e-05, eta: 2:32:19, time: 0.315, data_time: 0.048, memory: 5537, decode.loss_ce: 0.4271, decode.acc_seg: 76.1605, loss: 0.4271
2022-11-29 17:10:32,232 - mmseg - INFO - Iter [8450/40000]	lr: 4.733e-05, eta: 2:32:02, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3210, decode.acc_seg: 78.2300, loss: 0.3210
2022-11-29 17:10:45,796 - mmseg - INFO - Iter [8500/40000]	lr: 4.725e-05, eta: 2:31:44, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3686, decode.acc_seg: 79.0737, loss: 0.3686
2022-11-29 17:10:59,374 - mmseg - INFO - Iter [8550/40000]	lr: 4.718e-05, eta: 2:31:26, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3912, decode.acc_seg: 77.5740, loss: 0.3912
2022-11-29 17:11:12,930 - mmseg - INFO - Iter [8600/40000]	lr: 4.710e-05, eta: 2:31:08, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3534, decode.acc_seg: 78.2533, loss: 0.3534
2022-11-29 17:11:26,471 - mmseg - INFO - Iter [8650/40000]	lr: 4.703e-05, eta: 2:30:51, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3680, decode.acc_seg: 75.5575, loss: 0.3680
2022-11-29 17:11:39,990 - mmseg - INFO - Iter [8700/40000]	lr: 4.695e-05, eta: 2:30:33, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3753, decode.acc_seg: 73.9568, loss: 0.3753
2022-11-29 17:11:53,522 - mmseg - INFO - Iter [8750/40000]	lr: 4.688e-05, eta: 2:30:15, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3101, decode.acc_seg: 77.2446, loss: 0.3101
2022-11-29 17:12:07,104 - mmseg - INFO - Iter [8800/40000]	lr: 4.680e-05, eta: 2:29:58, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3064, decode.acc_seg: 79.8383, loss: 0.3064
2022-11-29 17:12:20,714 - mmseg - INFO - Iter [8850/40000]	lr: 4.673e-05, eta: 2:29:41, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3366, decode.acc_seg: 80.5721, loss: 0.3366
2022-11-29 17:12:34,290 - mmseg - INFO - Iter [8900/40000]	lr: 4.665e-05, eta: 2:29:23, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4332, decode.acc_seg: 73.0439, loss: 0.4332
2022-11-29 17:12:47,833 - mmseg - INFO - Iter [8950/40000]	lr: 4.658e-05, eta: 2:29:06, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3711, decode.acc_seg: 70.1596, loss: 0.3711
2022-11-29 17:13:01,391 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:13:01,391 - mmseg - INFO - Iter [9000/40000]	lr: 4.650e-05, eta: 2:28:49, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2882, decode.acc_seg: 83.9757, loss: 0.2882
2022-11-29 17:13:14,952 - mmseg - INFO - Iter [9050/40000]	lr: 4.643e-05, eta: 2:28:31, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3141, decode.acc_seg: 77.4118, loss: 0.3141
2022-11-29 17:13:29,645 - mmseg - INFO - Iter [9100/40000]	lr: 4.635e-05, eta: 2:28:18, time: 0.294, data_time: 0.019, memory: 5537, decode.loss_ce: 0.3359, decode.acc_seg: 79.0254, loss: 0.3359
2022-11-29 17:13:45,258 - mmseg - INFO - Iter [9150/40000]	lr: 4.628e-05, eta: 2:28:08, time: 0.312, data_time: 0.008, memory: 5537, decode.loss_ce: 0.4056, decode.acc_seg: 71.4196, loss: 0.4056
2022-11-29 17:13:59,347 - mmseg - INFO - Iter [9200/40000]	lr: 4.620e-05, eta: 2:27:52, time: 0.282, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4022, decode.acc_seg: 74.1638, loss: 0.4022
2022-11-29 17:14:12,875 - mmseg - INFO - Iter [9250/40000]	lr: 4.613e-05, eta: 2:27:35, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3176, decode.acc_seg: 76.1633, loss: 0.3176
2022-11-29 17:14:28,627 - mmseg - INFO - Iter [9300/40000]	lr: 4.605e-05, eta: 2:27:25, time: 0.315, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3348, decode.acc_seg: 79.3904, loss: 0.3348
2022-11-29 17:14:44,741 - mmseg - INFO - Iter [9350/40000]	lr: 4.598e-05, eta: 2:27:16, time: 0.322, data_time: 0.048, memory: 5537, decode.loss_ce: 0.3860, decode.acc_seg: 75.8220, loss: 0.3860
2022-11-29 17:14:58,365 - mmseg - INFO - Iter [9400/40000]	lr: 4.590e-05, eta: 2:26:59, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3461, decode.acc_seg: 69.9793, loss: 0.3461
2022-11-29 17:15:12,071 - mmseg - INFO - Iter [9450/40000]	lr: 4.583e-05, eta: 2:26:42, time: 0.274, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3452, decode.acc_seg: 77.5884, loss: 0.3452
2022-11-29 17:15:28,485 - mmseg - INFO - Iter [9500/40000]	lr: 4.575e-05, eta: 2:26:34, time: 0.328, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4287, decode.acc_seg: 69.4350, loss: 0.4287
2022-11-29 17:15:44,896 - mmseg - INFO - Iter [9550/40000]	lr: 4.568e-05, eta: 2:26:26, time: 0.328, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2771, decode.acc_seg: 79.0958, loss: 0.2771
2022-11-29 17:15:59,695 - mmseg - INFO - Iter [9600/40000]	lr: 4.560e-05, eta: 2:26:13, time: 0.296, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3158, decode.acc_seg: 83.7865, loss: 0.3158
2022-11-29 17:16:13,263 - mmseg - INFO - Iter [9650/40000]	lr: 4.553e-05, eta: 2:25:56, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3119, decode.acc_seg: 79.6821, loss: 0.3119
2022-11-29 17:16:26,804 - mmseg - INFO - Iter [9700/40000]	lr: 4.545e-05, eta: 2:25:39, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3964, decode.acc_seg: 71.1334, loss: 0.3964
2022-11-29 17:16:40,364 - mmseg - INFO - Iter [9750/40000]	lr: 4.538e-05, eta: 2:25:22, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3598, decode.acc_seg: 76.2505, loss: 0.3598
2022-11-29 17:16:53,915 - mmseg - INFO - Iter [9800/40000]	lr: 4.530e-05, eta: 2:25:05, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4308, decode.acc_seg: 72.4862, loss: 0.4308
2022-11-29 17:17:07,442 - mmseg - INFO - Iter [9850/40000]	lr: 4.523e-05, eta: 2:24:47, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2917, decode.acc_seg: 81.4863, loss: 0.2917
2022-11-29 17:17:21,045 - mmseg - INFO - Iter [9900/40000]	lr: 4.515e-05, eta: 2:24:31, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2883, decode.acc_seg: 80.5350, loss: 0.2883
2022-11-29 17:17:34,660 - mmseg - INFO - Iter [9950/40000]	lr: 4.508e-05, eta: 2:24:14, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.5036, decode.acc_seg: 67.7936, loss: 0.5036
2022-11-29 17:17:48,205 - mmseg - INFO - Saving checkpoint at 10000 iterations
2022-11-29 17:17:50,108 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:17:50,108 - mmseg - INFO - Iter [10000/40000]	lr: 4.500e-05, eta: 2:24:03, time: 0.309, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3663, decode.acc_seg: 73.0886, loss: 0.3663
2022-11-29 17:19:19,409 - mmseg - INFO - per class results:
2022-11-29 17:19:19,409 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
| background | 64.34 | 88.05 |
|  building  |  5.42 |  6.07 |
|  woodland  | 43.69 | 50.13 |
|   water    |  4.61 |  7.43 |
|    road    |  5.62 |  6.32 |
+------------+-------+-------+
2022-11-29 17:19:19,409 - mmseg - INFO - Summary:
2022-11-29 17:19:19,410 - mmseg - INFO - 
+-------+-------+------+
|  aAcc |  mIoU | mAcc |
+-------+-------+------+
| 69.29 | 24.74 | 31.6 |
+-------+-------+------+
2022-11-29 17:19:19,413 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:19:19,413 - mmseg - INFO - Iter(val) [1602]	aAcc: 0.6929, mIoU: 0.2474, mAcc: 0.3160, IoU.background: 0.6434, IoU.building: 0.0542, IoU.woodland: 0.4369, IoU.water: 0.0461, IoU.road: 0.0562, Acc.background: 0.8805, Acc.building: 0.0607, Acc.woodland: 0.5013, Acc.water: 0.0743, Acc.road: 0.0632
2022-11-29 17:19:34,456 - mmseg - INFO - Iter [10050/40000]	lr: 4.493e-05, eta: 2:28:16, time: 2.087, data_time: 1.795, memory: 5537, decode.loss_ce: 0.3719, decode.acc_seg: 78.3481, loss: 0.3719
2022-11-29 17:19:49,758 - mmseg - INFO - Iter [10100/40000]	lr: 4.485e-05, eta: 2:28:03, time: 0.306, data_time: 0.008, memory: 5537, decode.loss_ce: 0.3509, decode.acc_seg: 75.8329, loss: 0.3509
2022-11-29 17:20:05,774 - mmseg - INFO - Iter [10150/40000]	lr: 4.478e-05, eta: 2:27:51, time: 0.320, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3614, decode.acc_seg: 76.8972, loss: 0.3614
2022-11-29 17:20:19,307 - mmseg - INFO - Iter [10200/40000]	lr: 4.470e-05, eta: 2:27:33, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2887, decode.acc_seg: 78.3675, loss: 0.2887
2022-11-29 17:20:32,906 - mmseg - INFO - Iter [10250/40000]	lr: 4.463e-05, eta: 2:27:14, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4072, decode.acc_seg: 74.9386, loss: 0.4072
2022-11-29 17:20:49,650 - mmseg - INFO - Iter [10300/40000]	lr: 4.455e-05, eta: 2:27:05, time: 0.335, data_time: 0.049, memory: 5537, decode.loss_ce: 0.3233, decode.acc_seg: 80.0496, loss: 0.3233
2022-11-29 17:21:03,988 - mmseg - INFO - Iter [10350/40000]	lr: 4.448e-05, eta: 2:26:48, time: 0.287, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3095, decode.acc_seg: 77.0106, loss: 0.3095
2022-11-29 17:21:17,499 - mmseg - INFO - Iter [10400/40000]	lr: 4.440e-05, eta: 2:26:30, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3496, decode.acc_seg: 75.5839, loss: 0.3496
2022-11-29 17:21:31,009 - mmseg - INFO - Iter [10450/40000]	lr: 4.433e-05, eta: 2:26:11, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2782, decode.acc_seg: 79.8737, loss: 0.2782
2022-11-29 17:21:44,525 - mmseg - INFO - Iter [10500/40000]	lr: 4.425e-05, eta: 2:25:52, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2902, decode.acc_seg: 81.1497, loss: 0.2902
2022-11-29 17:21:58,055 - mmseg - INFO - Iter [10550/40000]	lr: 4.418e-05, eta: 2:25:34, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3197, decode.acc_seg: 79.6971, loss: 0.3197
2022-11-29 17:22:13,088 - mmseg - INFO - Iter [10600/40000]	lr: 4.410e-05, eta: 2:25:20, time: 0.301, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2896, decode.acc_seg: 82.3772, loss: 0.2896
2022-11-29 17:22:28,587 - mmseg - INFO - Iter [10650/40000]	lr: 4.403e-05, eta: 2:25:07, time: 0.310, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2895, decode.acc_seg: 84.5499, loss: 0.2895
2022-11-29 17:22:43,817 - mmseg - INFO - Iter [10700/40000]	lr: 4.395e-05, eta: 2:24:53, time: 0.305, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3011, decode.acc_seg: 78.8989, loss: 0.3011
2022-11-29 17:22:57,351 - mmseg - INFO - Iter [10750/40000]	lr: 4.388e-05, eta: 2:24:34, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3684, decode.acc_seg: 78.8015, loss: 0.3684
2022-11-29 17:23:10,858 - mmseg - INFO - Iter [10800/40000]	lr: 4.380e-05, eta: 2:24:16, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3538, decode.acc_seg: 76.3084, loss: 0.3538
2022-11-29 17:23:24,290 - mmseg - INFO - Iter [10850/40000]	lr: 4.373e-05, eta: 2:23:58, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3797, decode.acc_seg: 79.6090, loss: 0.3797
2022-11-29 17:23:37,819 - mmseg - INFO - Iter [10900/40000]	lr: 4.365e-05, eta: 2:23:39, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3628, decode.acc_seg: 73.1115, loss: 0.3628
2022-11-29 17:23:51,268 - mmseg - INFO - Iter [10950/40000]	lr: 4.358e-05, eta: 2:23:21, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3900, decode.acc_seg: 77.8602, loss: 0.3900
2022-11-29 17:24:04,693 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:24:04,693 - mmseg - INFO - Iter [11000/40000]	lr: 4.350e-05, eta: 2:23:02, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3129, decode.acc_seg: 76.6612, loss: 0.3129
2022-11-29 17:24:18,080 - mmseg - INFO - Iter [11050/40000]	lr: 4.343e-05, eta: 2:22:44, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3604, decode.acc_seg: 77.9717, loss: 0.3604
2022-11-29 17:24:31,481 - mmseg - INFO - Iter [11100/40000]	lr: 4.335e-05, eta: 2:22:25, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3910, decode.acc_seg: 75.9032, loss: 0.3910
2022-11-29 17:24:44,879 - mmseg - INFO - Iter [11150/40000]	lr: 4.328e-05, eta: 2:22:07, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3796, decode.acc_seg: 75.6005, loss: 0.3796
2022-11-29 17:25:00,486 - mmseg - INFO - Iter [11200/40000]	lr: 4.320e-05, eta: 2:21:54, time: 0.312, data_time: 0.048, memory: 5537, decode.loss_ce: 0.2617, decode.acc_seg: 81.8049, loss: 0.2617
2022-11-29 17:25:14,502 - mmseg - INFO - Iter [11250/40000]	lr: 4.313e-05, eta: 2:21:38, time: 0.280, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3085, decode.acc_seg: 79.3256, loss: 0.3085
2022-11-29 17:25:27,807 - mmseg - INFO - Iter [11300/40000]	lr: 4.305e-05, eta: 2:21:19, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3121, decode.acc_seg: 76.5504, loss: 0.3121
2022-11-29 17:25:41,122 - mmseg - INFO - Iter [11350/40000]	lr: 4.298e-05, eta: 2:21:01, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.4049, decode.acc_seg: 77.1478, loss: 0.4049
2022-11-29 17:25:54,468 - mmseg - INFO - Iter [11400/40000]	lr: 4.290e-05, eta: 2:20:42, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3464, decode.acc_seg: 73.1919, loss: 0.3464
2022-11-29 17:26:07,805 - mmseg - INFO - Iter [11450/40000]	lr: 4.283e-05, eta: 2:20:24, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3008, decode.acc_seg: 82.9758, loss: 0.3008
2022-11-29 17:26:21,105 - mmseg - INFO - Iter [11500/40000]	lr: 4.275e-05, eta: 2:20:06, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3220, decode.acc_seg: 78.6475, loss: 0.3220
2022-11-29 17:26:34,440 - mmseg - INFO - Iter [11550/40000]	lr: 4.268e-05, eta: 2:19:48, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3342, decode.acc_seg: 78.8161, loss: 0.3342
2022-11-29 17:26:47,875 - mmseg - INFO - Iter [11600/40000]	lr: 4.260e-05, eta: 2:19:30, time: 0.269, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2585, decode.acc_seg: 79.7496, loss: 0.2585
2022-11-29 17:27:01,249 - mmseg - INFO - Iter [11650/40000]	lr: 4.253e-05, eta: 2:19:12, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2752, decode.acc_seg: 83.7303, loss: 0.2752
2022-11-29 17:27:14,639 - mmseg - INFO - Iter [11700/40000]	lr: 4.245e-05, eta: 2:18:54, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2906, decode.acc_seg: 82.2086, loss: 0.2906
2022-11-29 17:27:28,112 - mmseg - INFO - Iter [11750/40000]	lr: 4.238e-05, eta: 2:18:36, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3836, decode.acc_seg: 77.1996, loss: 0.3836
2022-11-29 17:27:41,551 - mmseg - INFO - Iter [11800/40000]	lr: 4.230e-05, eta: 2:18:18, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2939, decode.acc_seg: 82.3384, loss: 0.2939
2022-11-29 17:27:55,170 - mmseg - INFO - Iter [11850/40000]	lr: 4.223e-05, eta: 2:18:01, time: 0.272, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3399, decode.acc_seg: 81.8355, loss: 0.3399
2022-11-29 17:28:09,402 - mmseg - INFO - Iter [11900/40000]	lr: 4.215e-05, eta: 2:17:45, time: 0.285, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3256, decode.acc_seg: 80.2398, loss: 0.3256
2022-11-29 17:28:22,945 - mmseg - INFO - Iter [11950/40000]	lr: 4.208e-05, eta: 2:17:27, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2786, decode.acc_seg: 78.7113, loss: 0.2786
2022-11-29 17:28:36,292 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:28:36,292 - mmseg - INFO - Iter [12000/40000]	lr: 4.200e-05, eta: 2:17:10, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3589, decode.acc_seg: 78.5214, loss: 0.3589
2022-11-29 17:28:49,681 - mmseg - INFO - Iter [12050/40000]	lr: 4.193e-05, eta: 2:16:52, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4065, decode.acc_seg: 78.6229, loss: 0.4065
2022-11-29 17:29:03,069 - mmseg - INFO - Iter [12100/40000]	lr: 4.185e-05, eta: 2:16:34, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.4803, decode.acc_seg: 72.1313, loss: 0.4803
2022-11-29 17:29:19,155 - mmseg - INFO - Iter [12150/40000]	lr: 4.178e-05, eta: 2:16:23, time: 0.322, data_time: 0.049, memory: 5537, decode.loss_ce: 0.2492, decode.acc_seg: 84.8781, loss: 0.2492
2022-11-29 17:29:32,460 - mmseg - INFO - Iter [12200/40000]	lr: 4.170e-05, eta: 2:16:05, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3442, decode.acc_seg: 78.8953, loss: 0.3442
2022-11-29 17:29:45,808 - mmseg - INFO - Iter [12250/40000]	lr: 4.163e-05, eta: 2:15:47, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3641, decode.acc_seg: 77.0458, loss: 0.3641
2022-11-29 17:29:59,218 - mmseg - INFO - Iter [12300/40000]	lr: 4.155e-05, eta: 2:15:30, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2998, decode.acc_seg: 79.4590, loss: 0.2998
2022-11-29 17:30:12,631 - mmseg - INFO - Iter [12350/40000]	lr: 4.148e-05, eta: 2:15:12, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3259, decode.acc_seg: 80.1541, loss: 0.3259
2022-11-29 17:30:26,054 - mmseg - INFO - Iter [12400/40000]	lr: 4.140e-05, eta: 2:14:55, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3127, decode.acc_seg: 79.9121, loss: 0.3127
2022-11-29 17:30:39,448 - mmseg - INFO - Iter [12450/40000]	lr: 4.133e-05, eta: 2:14:37, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3489, decode.acc_seg: 76.0415, loss: 0.3489
2022-11-29 17:30:52,793 - mmseg - INFO - Iter [12500/40000]	lr: 4.125e-05, eta: 2:14:20, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3303, decode.acc_seg: 78.9945, loss: 0.3303
2022-11-29 17:31:06,130 - mmseg - INFO - Iter [12550/40000]	lr: 4.118e-05, eta: 2:14:02, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3952, decode.acc_seg: 80.3090, loss: 0.3952
2022-11-29 17:31:19,485 - mmseg - INFO - Iter [12600/40000]	lr: 4.110e-05, eta: 2:13:45, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2338, decode.acc_seg: 85.0314, loss: 0.2338
2022-11-29 17:31:32,919 - mmseg - INFO - Iter [12650/40000]	lr: 4.103e-05, eta: 2:13:27, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3373, decode.acc_seg: 76.9801, loss: 0.3373
2022-11-29 17:31:46,304 - mmseg - INFO - Iter [12700/40000]	lr: 4.095e-05, eta: 2:13:10, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3406, decode.acc_seg: 80.3909, loss: 0.3406
2022-11-29 17:31:59,693 - mmseg - INFO - Iter [12750/40000]	lr: 4.088e-05, eta: 2:12:53, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2618, decode.acc_seg: 85.5780, loss: 0.2618
2022-11-29 17:32:13,143 - mmseg - INFO - Iter [12800/40000]	lr: 4.080e-05, eta: 2:12:36, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3590, decode.acc_seg: 76.5425, loss: 0.3590
2022-11-29 17:32:26,557 - mmseg - INFO - Iter [12850/40000]	lr: 4.073e-05, eta: 2:12:18, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3635, decode.acc_seg: 77.7588, loss: 0.3635
2022-11-29 17:32:39,921 - mmseg - INFO - Iter [12900/40000]	lr: 4.065e-05, eta: 2:12:01, time: 0.267, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2973, decode.acc_seg: 79.8420, loss: 0.2973
2022-11-29 17:32:53,324 - mmseg - INFO - Iter [12950/40000]	lr: 4.058e-05, eta: 2:11:44, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3708, decode.acc_seg: 74.5820, loss: 0.3708
2022-11-29 17:33:06,714 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:33:06,714 - mmseg - INFO - Iter [13000/40000]	lr: 4.050e-05, eta: 2:11:27, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3241, decode.acc_seg: 83.1128, loss: 0.3241
2022-11-29 17:33:20,005 - mmseg - INFO - Iter [13050/40000]	lr: 4.043e-05, eta: 2:11:09, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3104, decode.acc_seg: 80.8657, loss: 0.3104
2022-11-29 17:33:35,463 - mmseg - INFO - Iter [13100/40000]	lr: 4.035e-05, eta: 2:10:57, time: 0.309, data_time: 0.047, memory: 5537, decode.loss_ce: 0.3293, decode.acc_seg: 78.1719, loss: 0.3293
2022-11-29 17:33:48,893 - mmseg - INFO - Iter [13150/40000]	lr: 4.028e-05, eta: 2:10:40, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3005, decode.acc_seg: 78.5523, loss: 0.3005
2022-11-29 17:34:02,239 - mmseg - INFO - Iter [13200/40000]	lr: 4.020e-05, eta: 2:10:22, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2643, decode.acc_seg: 77.8985, loss: 0.2643
2022-11-29 17:34:15,579 - mmseg - INFO - Iter [13250/40000]	lr: 4.013e-05, eta: 2:10:05, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3473, decode.acc_seg: 77.1416, loss: 0.3473
2022-11-29 17:34:29,003 - mmseg - INFO - Iter [13300/40000]	lr: 4.005e-05, eta: 2:09:48, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3515, decode.acc_seg: 73.5973, loss: 0.3515
2022-11-29 17:34:42,521 - mmseg - INFO - Iter [13350/40000]	lr: 3.998e-05, eta: 2:09:32, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3155, decode.acc_seg: 77.8256, loss: 0.3155
2022-11-29 17:34:55,851 - mmseg - INFO - Iter [13400/40000]	lr: 3.990e-05, eta: 2:09:15, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3620, decode.acc_seg: 77.5777, loss: 0.3620
2022-11-29 17:35:09,193 - mmseg - INFO - Iter [13450/40000]	lr: 3.983e-05, eta: 2:08:58, time: 0.267, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2536, decode.acc_seg: 80.8084, loss: 0.2536
2022-11-29 17:35:22,577 - mmseg - INFO - Iter [13500/40000]	lr: 3.975e-05, eta: 2:08:41, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2926, decode.acc_seg: 86.3711, loss: 0.2926
2022-11-29 17:35:36,378 - mmseg - INFO - Iter [13550/40000]	lr: 3.968e-05, eta: 2:08:25, time: 0.276, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2470, decode.acc_seg: 84.8363, loss: 0.2470
2022-11-29 17:35:50,373 - mmseg - INFO - Iter [13600/40000]	lr: 3.960e-05, eta: 2:08:09, time: 0.280, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3275, decode.acc_seg: 77.8458, loss: 0.3275
2022-11-29 17:36:03,831 - mmseg - INFO - Iter [13650/40000]	lr: 3.953e-05, eta: 2:07:52, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3824, decode.acc_seg: 74.0664, loss: 0.3824
2022-11-29 17:36:17,305 - mmseg - INFO - Iter [13700/40000]	lr: 3.945e-05, eta: 2:07:36, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2915, decode.acc_seg: 84.6637, loss: 0.2915
2022-11-29 17:36:30,681 - mmseg - INFO - Iter [13750/40000]	lr: 3.938e-05, eta: 2:07:19, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3106, decode.acc_seg: 77.1527, loss: 0.3106
2022-11-29 17:36:44,059 - mmseg - INFO - Iter [13800/40000]	lr: 3.930e-05, eta: 2:07:02, time: 0.267, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3213, decode.acc_seg: 81.6436, loss: 0.3213
2022-11-29 17:36:57,494 - mmseg - INFO - Iter [13850/40000]	lr: 3.923e-05, eta: 2:06:45, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3188, decode.acc_seg: 77.8271, loss: 0.3188
2022-11-29 17:37:10,890 - mmseg - INFO - Iter [13900/40000]	lr: 3.915e-05, eta: 2:06:29, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2267, decode.acc_seg: 87.0150, loss: 0.2267
2022-11-29 17:37:24,273 - mmseg - INFO - Iter [13950/40000]	lr: 3.908e-05, eta: 2:06:12, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3949, decode.acc_seg: 75.9130, loss: 0.3949
2022-11-29 17:37:39,785 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:37:39,785 - mmseg - INFO - Iter [14000/40000]	lr: 3.900e-05, eta: 2:05:59, time: 0.310, data_time: 0.047, memory: 5537, decode.loss_ce: 0.3387, decode.acc_seg: 74.7992, loss: 0.3387
2022-11-29 17:37:53,737 - mmseg - INFO - Iter [14050/40000]	lr: 3.893e-05, eta: 2:05:44, time: 0.279, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3284, decode.acc_seg: 79.6985, loss: 0.3284
2022-11-29 17:38:08,303 - mmseg - INFO - Iter [14100/40000]	lr: 3.885e-05, eta: 2:05:29, time: 0.291, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3337, decode.acc_seg: 82.2621, loss: 0.3337
2022-11-29 17:38:21,805 - mmseg - INFO - Iter [14150/40000]	lr: 3.878e-05, eta: 2:05:13, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2328, decode.acc_seg: 83.4044, loss: 0.2328
2022-11-29 17:38:35,275 - mmseg - INFO - Iter [14200/40000]	lr: 3.870e-05, eta: 2:04:56, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2835, decode.acc_seg: 83.4104, loss: 0.2835
2022-11-29 17:38:48,766 - mmseg - INFO - Iter [14250/40000]	lr: 3.863e-05, eta: 2:04:40, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2489, decode.acc_seg: 83.4112, loss: 0.2489
2022-11-29 17:39:02,161 - mmseg - INFO - Iter [14300/40000]	lr: 3.855e-05, eta: 2:04:23, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2463, decode.acc_seg: 84.8565, loss: 0.2463
2022-11-29 17:39:15,584 - mmseg - INFO - Iter [14350/40000]	lr: 3.848e-05, eta: 2:04:07, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2597, decode.acc_seg: 85.7647, loss: 0.2597
2022-11-29 17:39:29,062 - mmseg - INFO - Iter [14400/40000]	lr: 3.840e-05, eta: 2:03:50, time: 0.270, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3673, decode.acc_seg: 73.3236, loss: 0.3673
2022-11-29 17:39:42,476 - mmseg - INFO - Iter [14450/40000]	lr: 3.833e-05, eta: 2:03:34, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2544, decode.acc_seg: 79.3350, loss: 0.2544
2022-11-29 17:39:55,872 - mmseg - INFO - Iter [14500/40000]	lr: 3.825e-05, eta: 2:03:18, time: 0.268, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3798, decode.acc_seg: 71.8661, loss: 0.3798
2022-11-29 17:40:09,398 - mmseg - INFO - Iter [14550/40000]	lr: 3.818e-05, eta: 2:03:01, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2788, decode.acc_seg: 82.0896, loss: 0.2788
2022-11-29 17:40:22,769 - mmseg - INFO - Iter [14600/40000]	lr: 3.810e-05, eta: 2:02:45, time: 0.267, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2500, decode.acc_seg: 85.1601, loss: 0.2500
2022-11-29 17:40:36,167 - mmseg - INFO - Iter [14650/40000]	lr: 3.803e-05, eta: 2:02:28, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2966, decode.acc_seg: 80.7160, loss: 0.2966
2022-11-29 17:40:52,433 - mmseg - INFO - Iter [14700/40000]	lr: 3.795e-05, eta: 2:02:17, time: 0.325, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2289, decode.acc_seg: 83.3826, loss: 0.2289
2022-11-29 17:41:08,580 - mmseg - INFO - Iter [14750/40000]	lr: 3.788e-05, eta: 2:02:05, time: 0.323, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3481, decode.acc_seg: 78.1805, loss: 0.3481
2022-11-29 17:41:22,276 - mmseg - INFO - Iter [14800/40000]	lr: 3.780e-05, eta: 2:01:49, time: 0.274, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2409, decode.acc_seg: 84.7437, loss: 0.2409
2022-11-29 17:41:35,566 - mmseg - INFO - Iter [14850/40000]	lr: 3.773e-05, eta: 2:01:33, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2898, decode.acc_seg: 83.8914, loss: 0.2898
2022-11-29 17:41:48,791 - mmseg - INFO - Iter [14900/40000]	lr: 3.765e-05, eta: 2:01:16, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2526, decode.acc_seg: 82.7852, loss: 0.2526
2022-11-29 17:42:04,347 - mmseg - INFO - Iter [14950/40000]	lr: 3.758e-05, eta: 2:01:03, time: 0.311, data_time: 0.048, memory: 5537, decode.loss_ce: 0.2382, decode.acc_seg: 83.8141, loss: 0.2382
2022-11-29 17:42:17,431 - mmseg - INFO - Saving checkpoint at 15000 iterations
2022-11-29 17:42:20,169 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:42:20,170 - mmseg - INFO - Iter [15000/40000]	lr: 3.750e-05, eta: 2:00:51, time: 0.316, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3056, decode.acc_seg: 79.4713, loss: 0.3056
2022-11-29 17:43:45,922 - mmseg - INFO - per class results:
2022-11-29 17:43:45,923 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
| background | 65.08 | 93.03 |
|  building  |  0.13 |  0.13 |
|  woodland  | 35.01 | 36.23 |
|   water    | 14.96 |  25.5 |
|    road    |  3.7  |  3.89 |
+------------+-------+-------+
2022-11-29 17:43:45,923 - mmseg - INFO - Summary:
2022-11-29 17:43:45,923 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 68.86 | 23.77 | 31.76 |
+-------+-------+-------+
2022-11-29 17:43:45,925 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:43:45,925 - mmseg - INFO - Iter(val) [1602]	aAcc: 0.6886, mIoU: 0.2377, mAcc: 0.3176, IoU.background: 0.6508, IoU.building: 0.0013, IoU.woodland: 0.3501, IoU.water: 0.1496, IoU.road: 0.0370, Acc.background: 0.9303, Acc.building: 0.0013, Acc.woodland: 0.3623, Acc.water: 0.2550, Acc.road: 0.0389
2022-11-29 17:43:59,245 - mmseg - INFO - Iter [15050/40000]	lr: 3.743e-05, eta: 2:02:57, time: 1.981, data_time: 1.721, memory: 5537, decode.loss_ce: 0.2951, decode.acc_seg: 83.7233, loss: 0.2951
2022-11-29 17:44:13,891 - mmseg - INFO - Iter [15100/40000]	lr: 3.735e-05, eta: 2:02:42, time: 0.293, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2738, decode.acc_seg: 85.0910, loss: 0.2738
2022-11-29 17:44:26,934 - mmseg - INFO - Iter [15150/40000]	lr: 3.728e-05, eta: 2:02:24, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3065, decode.acc_seg: 78.1611, loss: 0.3065
2022-11-29 17:44:40,016 - mmseg - INFO - Iter [15200/40000]	lr: 3.720e-05, eta: 2:02:07, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3289, decode.acc_seg: 76.2969, loss: 0.3289
2022-11-29 17:44:53,321 - mmseg - INFO - Iter [15250/40000]	lr: 3.713e-05, eta: 2:01:50, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2942, decode.acc_seg: 80.8160, loss: 0.2942
2022-11-29 17:45:06,419 - mmseg - INFO - Iter [15300/40000]	lr: 3.705e-05, eta: 2:01:32, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2673, decode.acc_seg: 83.8319, loss: 0.2673
2022-11-29 17:45:19,502 - mmseg - INFO - Iter [15350/40000]	lr: 3.698e-05, eta: 2:01:15, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3109, decode.acc_seg: 78.7964, loss: 0.3109
2022-11-29 17:45:32,610 - mmseg - INFO - Iter [15400/40000]	lr: 3.690e-05, eta: 2:00:57, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3227, decode.acc_seg: 77.1232, loss: 0.3227
2022-11-29 17:45:45,758 - mmseg - INFO - Iter [15450/40000]	lr: 3.683e-05, eta: 2:00:40, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2801, decode.acc_seg: 80.8872, loss: 0.2801
2022-11-29 17:45:58,896 - mmseg - INFO - Iter [15500/40000]	lr: 3.675e-05, eta: 2:00:23, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2868, decode.acc_seg: 76.3864, loss: 0.2868
2022-11-29 17:46:11,962 - mmseg - INFO - Iter [15550/40000]	lr: 3.668e-05, eta: 2:00:05, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3124, decode.acc_seg: 79.8283, loss: 0.3124
2022-11-29 17:46:25,050 - mmseg - INFO - Iter [15600/40000]	lr: 3.660e-05, eta: 1:59:48, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2902, decode.acc_seg: 81.3528, loss: 0.2902
2022-11-29 17:46:38,137 - mmseg - INFO - Iter [15650/40000]	lr: 3.653e-05, eta: 1:59:31, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2774, decode.acc_seg: 82.0294, loss: 0.2774
2022-11-29 17:46:51,259 - mmseg - INFO - Iter [15700/40000]	lr: 3.645e-05, eta: 1:59:13, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2351, decode.acc_seg: 86.5553, loss: 0.2351
2022-11-29 17:47:04,372 - mmseg - INFO - Iter [15750/40000]	lr: 3.638e-05, eta: 1:58:56, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3045, decode.acc_seg: 79.9885, loss: 0.3045
2022-11-29 17:47:17,534 - mmseg - INFO - Iter [15800/40000]	lr: 3.630e-05, eta: 1:58:39, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3584, decode.acc_seg: 81.1812, loss: 0.3584
2022-11-29 17:47:30,577 - mmseg - INFO - Iter [15850/40000]	lr: 3.623e-05, eta: 1:58:22, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3495, decode.acc_seg: 76.8079, loss: 0.3495
2022-11-29 17:47:45,800 - mmseg - INFO - Iter [15900/40000]	lr: 3.615e-05, eta: 1:58:08, time: 0.304, data_time: 0.048, memory: 5537, decode.loss_ce: 0.3752, decode.acc_seg: 82.8291, loss: 0.3752
2022-11-29 17:47:58,819 - mmseg - INFO - Iter [15950/40000]	lr: 3.608e-05, eta: 1:57:51, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3516, decode.acc_seg: 80.4331, loss: 0.3516
2022-11-29 17:48:11,900 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:48:11,900 - mmseg - INFO - Iter [16000/40000]	lr: 3.600e-05, eta: 1:57:34, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3165, decode.acc_seg: 80.3558, loss: 0.3165
2022-11-29 17:48:25,045 - mmseg - INFO - Iter [16050/40000]	lr: 3.593e-05, eta: 1:57:17, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3201, decode.acc_seg: 79.2568, loss: 0.3201
2022-11-29 17:48:38,169 - mmseg - INFO - Iter [16100/40000]	lr: 3.585e-05, eta: 1:57:00, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3556, decode.acc_seg: 72.1033, loss: 0.3556
2022-11-29 17:48:51,393 - mmseg - INFO - Iter [16150/40000]	lr: 3.578e-05, eta: 1:56:43, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3077, decode.acc_seg: 84.4417, loss: 0.3077
2022-11-29 17:49:04,453 - mmseg - INFO - Iter [16200/40000]	lr: 3.570e-05, eta: 1:56:26, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2642, decode.acc_seg: 81.7549, loss: 0.2642
2022-11-29 17:49:17,596 - mmseg - INFO - Iter [16250/40000]	lr: 3.563e-05, eta: 1:56:09, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2700, decode.acc_seg: 81.4551, loss: 0.2700
2022-11-29 17:49:30,679 - mmseg - INFO - Iter [16300/40000]	lr: 3.555e-05, eta: 1:55:52, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3081, decode.acc_seg: 79.3664, loss: 0.3081
2022-11-29 17:49:43,815 - mmseg - INFO - Iter [16350/40000]	lr: 3.548e-05, eta: 1:55:35, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3302, decode.acc_seg: 81.8838, loss: 0.3302
2022-11-29 17:49:56,876 - mmseg - INFO - Iter [16400/40000]	lr: 3.540e-05, eta: 1:55:18, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2070, decode.acc_seg: 85.7714, loss: 0.2070
2022-11-29 17:50:09,925 - mmseg - INFO - Iter [16450/40000]	lr: 3.533e-05, eta: 1:55:01, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2103, decode.acc_seg: 88.1892, loss: 0.2103
2022-11-29 17:50:23,037 - mmseg - INFO - Iter [16500/40000]	lr: 3.525e-05, eta: 1:54:44, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3502, decode.acc_seg: 78.2845, loss: 0.3502
2022-11-29 17:50:36,136 - mmseg - INFO - Iter [16550/40000]	lr: 3.518e-05, eta: 1:54:27, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2270, decode.acc_seg: 85.5357, loss: 0.2270
2022-11-29 17:50:49,189 - mmseg - INFO - Iter [16600/40000]	lr: 3.510e-05, eta: 1:54:10, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3176, decode.acc_seg: 79.4191, loss: 0.3176
2022-11-29 17:51:02,232 - mmseg - INFO - Iter [16650/40000]	lr: 3.503e-05, eta: 1:53:53, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3087, decode.acc_seg: 83.3682, loss: 0.3087
2022-11-29 17:51:15,284 - mmseg - INFO - Iter [16700/40000]	lr: 3.495e-05, eta: 1:53:37, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2657, decode.acc_seg: 82.4583, loss: 0.2657
2022-11-29 17:51:28,291 - mmseg - INFO - Iter [16750/40000]	lr: 3.488e-05, eta: 1:53:20, time: 0.260, data_time: 0.005, memory: 5537, decode.loss_ce: 0.2480, decode.acc_seg: 85.4636, loss: 0.2480
2022-11-29 17:51:43,404 - mmseg - INFO - Iter [16800/40000]	lr: 3.480e-05, eta: 1:53:06, time: 0.302, data_time: 0.047, memory: 5537, decode.loss_ce: 0.2991, decode.acc_seg: 80.0544, loss: 0.2991
2022-11-29 17:51:56,467 - mmseg - INFO - Iter [16850/40000]	lr: 3.473e-05, eta: 1:52:49, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3737, decode.acc_seg: 75.6871, loss: 0.3737
2022-11-29 17:52:09,507 - mmseg - INFO - Iter [16900/40000]	lr: 3.465e-05, eta: 1:52:32, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2467, decode.acc_seg: 84.2565, loss: 0.2467
2022-11-29 17:52:22,598 - mmseg - INFO - Iter [16950/40000]	lr: 3.458e-05, eta: 1:52:16, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2126, decode.acc_seg: 89.5510, loss: 0.2126
2022-11-29 17:52:35,633 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:52:35,633 - mmseg - INFO - Iter [17000/40000]	lr: 3.450e-05, eta: 1:51:59, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2624, decode.acc_seg: 84.0067, loss: 0.2624
2022-11-29 17:52:48,710 - mmseg - INFO - Iter [17050/40000]	lr: 3.443e-05, eta: 1:51:42, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2416, decode.acc_seg: 82.3703, loss: 0.2416
2022-11-29 17:53:01,817 - mmseg - INFO - Iter [17100/40000]	lr: 3.435e-05, eta: 1:51:26, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3368, decode.acc_seg: 80.6531, loss: 0.3368
2022-11-29 17:53:15,087 - mmseg - INFO - Iter [17150/40000]	lr: 3.428e-05, eta: 1:51:09, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2756, decode.acc_seg: 82.5453, loss: 0.2756
2022-11-29 17:53:28,136 - mmseg - INFO - Iter [17200/40000]	lr: 3.420e-05, eta: 1:50:53, time: 0.261, data_time: 0.005, memory: 5537, decode.loss_ce: 0.2413, decode.acc_seg: 85.9318, loss: 0.2413
2022-11-29 17:53:41,251 - mmseg - INFO - Iter [17250/40000]	lr: 3.413e-05, eta: 1:50:36, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2265, decode.acc_seg: 85.1638, loss: 0.2265
2022-11-29 17:53:54,298 - mmseg - INFO - Iter [17300/40000]	lr: 3.405e-05, eta: 1:50:19, time: 0.261, data_time: 0.005, memory: 5537, decode.loss_ce: 0.3895, decode.acc_seg: 75.6787, loss: 0.3895
2022-11-29 17:54:07,337 - mmseg - INFO - Iter [17350/40000]	lr: 3.398e-05, eta: 1:50:03, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3219, decode.acc_seg: 80.0341, loss: 0.3219
2022-11-29 17:54:20,442 - mmseg - INFO - Iter [17400/40000]	lr: 3.390e-05, eta: 1:49:46, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3092, decode.acc_seg: 79.0273, loss: 0.3092
2022-11-29 17:54:33,498 - mmseg - INFO - Iter [17450/40000]	lr: 3.383e-05, eta: 1:49:30, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3297, decode.acc_seg: 79.3064, loss: 0.3297
2022-11-29 17:54:46,577 - mmseg - INFO - Iter [17500/40000]	lr: 3.375e-05, eta: 1:49:13, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3308, decode.acc_seg: 75.4376, loss: 0.3308
2022-11-29 17:54:59,690 - mmseg - INFO - Iter [17550/40000]	lr: 3.368e-05, eta: 1:48:57, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2357, decode.acc_seg: 85.3382, loss: 0.2357
2022-11-29 17:55:12,774 - mmseg - INFO - Iter [17600/40000]	lr: 3.360e-05, eta: 1:48:40, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2287, decode.acc_seg: 84.2807, loss: 0.2287
2022-11-29 17:55:25,808 - mmseg - INFO - Iter [17650/40000]	lr: 3.353e-05, eta: 1:48:24, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3209, decode.acc_seg: 78.2570, loss: 0.3209
2022-11-29 17:55:38,900 - mmseg - INFO - Iter [17700/40000]	lr: 3.345e-05, eta: 1:48:08, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2535, decode.acc_seg: 83.6748, loss: 0.2535
2022-11-29 17:55:54,159 - mmseg - INFO - Iter [17750/40000]	lr: 3.338e-05, eta: 1:47:54, time: 0.305, data_time: 0.048, memory: 5537, decode.loss_ce: 0.2130, decode.acc_seg: 85.2921, loss: 0.2130
2022-11-29 17:56:07,196 - mmseg - INFO - Iter [17800/40000]	lr: 3.330e-05, eta: 1:47:37, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2822, decode.acc_seg: 82.6018, loss: 0.2822
2022-11-29 17:56:20,255 - mmseg - INFO - Iter [17850/40000]	lr: 3.323e-05, eta: 1:47:21, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3982, decode.acc_seg: 76.7788, loss: 0.3982
2022-11-29 17:56:33,359 - mmseg - INFO - Iter [17900/40000]	lr: 3.315e-05, eta: 1:47:05, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2984, decode.acc_seg: 81.6725, loss: 0.2984
2022-11-29 17:56:46,569 - mmseg - INFO - Iter [17950/40000]	lr: 3.308e-05, eta: 1:46:49, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2883, decode.acc_seg: 80.6159, loss: 0.2883
2022-11-29 17:56:59,688 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 17:56:59,688 - mmseg - INFO - Iter [18000/40000]	lr: 3.300e-05, eta: 1:46:32, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3142, decode.acc_seg: 79.3144, loss: 0.3142
2022-11-29 17:57:12,773 - mmseg - INFO - Iter [18050/40000]	lr: 3.293e-05, eta: 1:46:16, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1958, decode.acc_seg: 86.6894, loss: 0.1958
2022-11-29 17:57:25,828 - mmseg - INFO - Iter [18100/40000]	lr: 3.285e-05, eta: 1:46:00, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1968, decode.acc_seg: 86.9400, loss: 0.1968
2022-11-29 17:57:41,883 - mmseg - INFO - Iter [18150/40000]	lr: 3.278e-05, eta: 1:45:47, time: 0.321, data_time: 0.064, memory: 5537, decode.loss_ce: 0.2860, decode.acc_seg: 82.8769, loss: 0.2860
2022-11-29 17:57:54,884 - mmseg - INFO - Iter [18200/40000]	lr: 3.270e-05, eta: 1:45:31, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2555, decode.acc_seg: 84.0135, loss: 0.2555
2022-11-29 17:58:07,903 - mmseg - INFO - Iter [18250/40000]	lr: 3.263e-05, eta: 1:45:14, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3277, decode.acc_seg: 77.1458, loss: 0.3277
2022-11-29 17:58:20,971 - mmseg - INFO - Iter [18300/40000]	lr: 3.255e-05, eta: 1:44:58, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3111, decode.acc_seg: 77.8120, loss: 0.3111
2022-11-29 17:58:33,958 - mmseg - INFO - Iter [18350/40000]	lr: 3.248e-05, eta: 1:44:42, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2196, decode.acc_seg: 84.7209, loss: 0.2196
2022-11-29 17:58:47,089 - mmseg - INFO - Iter [18400/40000]	lr: 3.240e-05, eta: 1:44:26, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3454, decode.acc_seg: 78.5946, loss: 0.3454
2022-11-29 17:59:00,164 - mmseg - INFO - Iter [18450/40000]	lr: 3.233e-05, eta: 1:44:10, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2760, decode.acc_seg: 79.7605, loss: 0.2760
2022-11-29 17:59:13,248 - mmseg - INFO - Iter [18500/40000]	lr: 3.225e-05, eta: 1:43:53, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2836, decode.acc_seg: 80.3868, loss: 0.2836
2022-11-29 17:59:26,314 - mmseg - INFO - Iter [18550/40000]	lr: 3.218e-05, eta: 1:43:37, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2958, decode.acc_seg: 79.2182, loss: 0.2958
2022-11-29 17:59:39,391 - mmseg - INFO - Iter [18600/40000]	lr: 3.210e-05, eta: 1:43:21, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3593, decode.acc_seg: 80.7014, loss: 0.3593
2022-11-29 17:59:52,532 - mmseg - INFO - Iter [18650/40000]	lr: 3.203e-05, eta: 1:43:05, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2985, decode.acc_seg: 78.8546, loss: 0.2985
2022-11-29 18:00:08,129 - mmseg - INFO - Iter [18700/40000]	lr: 3.195e-05, eta: 1:42:52, time: 0.312, data_time: 0.049, memory: 5537, decode.loss_ce: 0.2682, decode.acc_seg: 83.4022, loss: 0.2682
2022-11-29 18:00:21,181 - mmseg - INFO - Iter [18750/40000]	lr: 3.188e-05, eta: 1:42:36, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2864, decode.acc_seg: 84.2439, loss: 0.2864
2022-11-29 18:00:34,319 - mmseg - INFO - Iter [18800/40000]	lr: 3.180e-05, eta: 1:42:20, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2735, decode.acc_seg: 81.7791, loss: 0.2735
2022-11-29 18:00:47,380 - mmseg - INFO - Iter [18850/40000]	lr: 3.173e-05, eta: 1:42:04, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2508, decode.acc_seg: 84.3614, loss: 0.2508
2022-11-29 18:01:00,575 - mmseg - INFO - Iter [18900/40000]	lr: 3.165e-05, eta: 1:41:48, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2545, decode.acc_seg: 81.5401, loss: 0.2545
2022-11-29 18:01:13,640 - mmseg - INFO - Iter [18950/40000]	lr: 3.158e-05, eta: 1:41:32, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2332, decode.acc_seg: 85.0825, loss: 0.2332
2022-11-29 18:01:26,955 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:01:26,955 - mmseg - INFO - Iter [19000/40000]	lr: 3.150e-05, eta: 1:41:16, time: 0.266, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2656, decode.acc_seg: 82.7578, loss: 0.2656
2022-11-29 18:01:42,107 - mmseg - INFO - Iter [19050/40000]	lr: 3.143e-05, eta: 1:41:02, time: 0.303, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2796, decode.acc_seg: 79.4554, loss: 0.2796
2022-11-29 18:01:55,139 - mmseg - INFO - Iter [19100/40000]	lr: 3.135e-05, eta: 1:40:46, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3610, decode.acc_seg: 79.4555, loss: 0.3610
2022-11-29 18:02:08,201 - mmseg - INFO - Iter [19150/40000]	lr: 3.128e-05, eta: 1:40:30, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2324, decode.acc_seg: 85.8689, loss: 0.2324
2022-11-29 18:02:21,296 - mmseg - INFO - Iter [19200/40000]	lr: 3.120e-05, eta: 1:40:14, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3208, decode.acc_seg: 84.6393, loss: 0.3208
2022-11-29 18:02:34,336 - mmseg - INFO - Iter [19250/40000]	lr: 3.113e-05, eta: 1:39:58, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3333, decode.acc_seg: 78.8025, loss: 0.3333
2022-11-29 18:02:47,391 - mmseg - INFO - Iter [19300/40000]	lr: 3.105e-05, eta: 1:39:42, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2478, decode.acc_seg: 83.2454, loss: 0.2478
2022-11-29 18:03:00,474 - mmseg - INFO - Iter [19350/40000]	lr: 3.098e-05, eta: 1:39:26, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2047, decode.acc_seg: 88.2206, loss: 0.2047
2022-11-29 18:03:13,511 - mmseg - INFO - Iter [19400/40000]	lr: 3.090e-05, eta: 1:39:10, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2385, decode.acc_seg: 85.4526, loss: 0.2385
2022-11-29 18:03:26,700 - mmseg - INFO - Iter [19450/40000]	lr: 3.083e-05, eta: 1:38:55, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3517, decode.acc_seg: 79.7685, loss: 0.3517
2022-11-29 18:03:39,774 - mmseg - INFO - Iter [19500/40000]	lr: 3.075e-05, eta: 1:38:39, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2865, decode.acc_seg: 79.1702, loss: 0.2865
2022-11-29 18:03:52,908 - mmseg - INFO - Iter [19550/40000]	lr: 3.068e-05, eta: 1:38:23, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2144, decode.acc_seg: 84.8210, loss: 0.2144
2022-11-29 18:04:08,177 - mmseg - INFO - Iter [19600/40000]	lr: 3.060e-05, eta: 1:38:09, time: 0.305, data_time: 0.048, memory: 5537, decode.loss_ce: 0.2917, decode.acc_seg: 80.5195, loss: 0.2917
2022-11-29 18:04:21,294 - mmseg - INFO - Iter [19650/40000]	lr: 3.053e-05, eta: 1:37:54, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2592, decode.acc_seg: 86.1920, loss: 0.2592
2022-11-29 18:04:34,438 - mmseg - INFO - Iter [19700/40000]	lr: 3.045e-05, eta: 1:37:38, time: 0.263, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2663, decode.acc_seg: 82.3950, loss: 0.2663
2022-11-29 18:04:47,635 - mmseg - INFO - Iter [19750/40000]	lr: 3.038e-05, eta: 1:37:22, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2373, decode.acc_seg: 84.0791, loss: 0.2373
2022-11-29 18:05:00,678 - mmseg - INFO - Iter [19800/40000]	lr: 3.030e-05, eta: 1:37:06, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2633, decode.acc_seg: 84.7204, loss: 0.2633
2022-11-29 18:05:13,739 - mmseg - INFO - Iter [19850/40000]	lr: 3.023e-05, eta: 1:36:51, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2611, decode.acc_seg: 87.9331, loss: 0.2611
2022-11-29 18:05:26,848 - mmseg - INFO - Iter [19900/40000]	lr: 3.015e-05, eta: 1:36:35, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2474, decode.acc_seg: 84.3757, loss: 0.2474
2022-11-29 18:05:39,893 - mmseg - INFO - Iter [19950/40000]	lr: 3.008e-05, eta: 1:36:19, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2688, decode.acc_seg: 84.0445, loss: 0.2688
2022-11-29 18:05:52,934 - mmseg - INFO - Saving checkpoint at 20000 iterations
2022-11-29 18:05:54,689 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:05:54,689 - mmseg - INFO - Iter [20000/40000]	lr: 3.000e-05, eta: 1:36:05, time: 0.296, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2857, decode.acc_seg: 83.5028, loss: 0.2857
2022-11-29 18:07:17,927 - mmseg - INFO - per class results:
2022-11-29 18:07:17,927 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
| background | 64.04 |  92.4 |
|  building  |  0.67 |  0.67 |
|  woodland  |  30.6 | 31.41 |
|   water    | 27.08 | 46.18 |
|    road    |  6.45 |  7.15 |
+------------+-------+-------+
2022-11-29 18:07:17,927 - mmseg - INFO - Summary:
2022-11-29 18:07:17,927 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 68.24 | 25.77 | 35.56 |
+-------+-------+-------+
2022-11-29 18:07:17,930 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:07:17,930 - mmseg - INFO - Iter(val) [1602]	aAcc: 0.6824, mIoU: 0.2577, mAcc: 0.3556, IoU.background: 0.6404, IoU.building: 0.0067, IoU.woodland: 0.3060, IoU.water: 0.2708, IoU.road: 0.0645, Acc.background: 0.9240, Acc.building: 0.0067, Acc.woodland: 0.3141, Acc.water: 0.4618, Acc.road: 0.0715
2022-11-29 18:07:31,043 - mmseg - INFO - Iter [20050/40000]	lr: 2.993e-05, eta: 1:37:12, time: 1.927, data_time: 1.671, memory: 5537, decode.loss_ce: 0.2658, decode.acc_seg: 86.1712, loss: 0.2658
2022-11-29 18:07:44,156 - mmseg - INFO - Iter [20100/40000]	lr: 2.985e-05, eta: 1:36:56, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2396, decode.acc_seg: 85.1504, loss: 0.2396
2022-11-29 18:07:57,426 - mmseg - INFO - Iter [20150/40000]	lr: 2.978e-05, eta: 1:36:40, time: 0.265, data_time: 0.008, memory: 5537, decode.loss_ce: 0.3053, decode.acc_seg: 80.0521, loss: 0.3053
2022-11-29 18:08:10,476 - mmseg - INFO - Iter [20200/40000]	lr: 2.970e-05, eta: 1:36:24, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2139, decode.acc_seg: 84.9952, loss: 0.2139
2022-11-29 18:08:23,541 - mmseg - INFO - Iter [20250/40000]	lr: 2.963e-05, eta: 1:36:08, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3092, decode.acc_seg: 81.1926, loss: 0.3092
2022-11-29 18:08:36,732 - mmseg - INFO - Iter [20300/40000]	lr: 2.955e-05, eta: 1:35:52, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2229, decode.acc_seg: 82.3813, loss: 0.2229
2022-11-29 18:08:49,856 - mmseg - INFO - Iter [20350/40000]	lr: 2.948e-05, eta: 1:35:36, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3434, decode.acc_seg: 74.9425, loss: 0.3434
2022-11-29 18:09:02,977 - mmseg - INFO - Iter [20400/40000]	lr: 2.940e-05, eta: 1:35:20, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2152, decode.acc_seg: 88.0507, loss: 0.2152
2022-11-29 18:09:16,075 - mmseg - INFO - Iter [20450/40000]	lr: 2.933e-05, eta: 1:35:04, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2907, decode.acc_seg: 80.6725, loss: 0.2907
2022-11-29 18:09:29,164 - mmseg - INFO - Iter [20500/40000]	lr: 2.925e-05, eta: 1:34:48, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3024, decode.acc_seg: 81.4699, loss: 0.3024
2022-11-29 18:09:44,718 - mmseg - INFO - Iter [20550/40000]	lr: 2.918e-05, eta: 1:34:34, time: 0.311, data_time: 0.047, memory: 5537, decode.loss_ce: 0.2322, decode.acc_seg: 84.5357, loss: 0.2322
2022-11-29 18:10:00,325 - mmseg - INFO - Iter [20600/40000]	lr: 2.910e-05, eta: 1:34:20, time: 0.312, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1883, decode.acc_seg: 88.6818, loss: 0.1883
2022-11-29 18:10:14,290 - mmseg - INFO - Iter [20650/40000]	lr: 2.903e-05, eta: 1:34:05, time: 0.279, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2977, decode.acc_seg: 79.8596, loss: 0.2977
2022-11-29 18:10:27,453 - mmseg - INFO - Iter [20700/40000]	lr: 2.895e-05, eta: 1:33:49, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3066, decode.acc_seg: 79.1361, loss: 0.3066
2022-11-29 18:10:40,551 - mmseg - INFO - Iter [20750/40000]	lr: 2.888e-05, eta: 1:33:33, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2563, decode.acc_seg: 83.4649, loss: 0.2563
2022-11-29 18:10:53,826 - mmseg - INFO - Iter [20800/40000]	lr: 2.880e-05, eta: 1:33:18, time: 0.265, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2405, decode.acc_seg: 84.1678, loss: 0.2405
2022-11-29 18:11:06,885 - mmseg - INFO - Iter [20850/40000]	lr: 2.873e-05, eta: 1:33:02, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3506, decode.acc_seg: 79.9510, loss: 0.3506
2022-11-29 18:11:20,004 - mmseg - INFO - Iter [20900/40000]	lr: 2.865e-05, eta: 1:32:46, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3051, decode.acc_seg: 83.5829, loss: 0.3051
2022-11-29 18:11:33,104 - mmseg - INFO - Iter [20950/40000]	lr: 2.858e-05, eta: 1:32:30, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2161, decode.acc_seg: 86.1331, loss: 0.2161
2022-11-29 18:11:46,227 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:11:46,228 - mmseg - INFO - Iter [21000/40000]	lr: 2.850e-05, eta: 1:32:14, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2334, decode.acc_seg: 83.7903, loss: 0.2334
2022-11-29 18:11:59,286 - mmseg - INFO - Iter [21050/40000]	lr: 2.843e-05, eta: 1:31:58, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2394, decode.acc_seg: 85.1260, loss: 0.2394
2022-11-29 18:12:13,580 - mmseg - INFO - Iter [21100/40000]	lr: 2.835e-05, eta: 1:31:43, time: 0.286, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2447, decode.acc_seg: 86.8499, loss: 0.2447
2022-11-29 18:12:29,338 - mmseg - INFO - Iter [21150/40000]	lr: 2.828e-05, eta: 1:31:30, time: 0.315, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2265, decode.acc_seg: 87.8917, loss: 0.2265
2022-11-29 18:12:42,602 - mmseg - INFO - Iter [21200/40000]	lr: 2.820e-05, eta: 1:31:14, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2450, decode.acc_seg: 85.2427, loss: 0.2450
2022-11-29 18:12:55,733 - mmseg - INFO - Iter [21250/40000]	lr: 2.813e-05, eta: 1:30:58, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3126, decode.acc_seg: 80.2309, loss: 0.3126
2022-11-29 18:13:08,830 - mmseg - INFO - Iter [21300/40000]	lr: 2.805e-05, eta: 1:30:42, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2169, decode.acc_seg: 85.7340, loss: 0.2169
2022-11-29 18:13:21,952 - mmseg - INFO - Iter [21350/40000]	lr: 2.798e-05, eta: 1:30:27, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2484, decode.acc_seg: 83.7053, loss: 0.2484
2022-11-29 18:13:35,129 - mmseg - INFO - Iter [21400/40000]	lr: 2.790e-05, eta: 1:30:11, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2511, decode.acc_seg: 83.6466, loss: 0.2511
2022-11-29 18:13:48,313 - mmseg - INFO - Iter [21450/40000]	lr: 2.783e-05, eta: 1:29:55, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2599, decode.acc_seg: 79.6939, loss: 0.2599
2022-11-29 18:14:03,808 - mmseg - INFO - Iter [21500/40000]	lr: 2.775e-05, eta: 1:29:41, time: 0.310, data_time: 0.048, memory: 5537, decode.loss_ce: 0.2194, decode.acc_seg: 86.5099, loss: 0.2194
2022-11-29 18:14:16,910 - mmseg - INFO - Iter [21550/40000]	lr: 2.768e-05, eta: 1:29:26, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2016, decode.acc_seg: 84.8853, loss: 0.2016
2022-11-29 18:14:29,952 - mmseg - INFO - Iter [21600/40000]	lr: 2.760e-05, eta: 1:29:10, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3400, decode.acc_seg: 77.8297, loss: 0.3400
2022-11-29 18:14:43,014 - mmseg - INFO - Iter [21650/40000]	lr: 2.753e-05, eta: 1:28:54, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2083, decode.acc_seg: 88.7920, loss: 0.2083
2022-11-29 18:14:56,117 - mmseg - INFO - Iter [21700/40000]	lr: 2.745e-05, eta: 1:28:38, time: 0.262, data_time: 0.005, memory: 5537, decode.loss_ce: 0.2818, decode.acc_seg: 81.0108, loss: 0.2818
2022-11-29 18:15:09,174 - mmseg - INFO - Iter [21750/40000]	lr: 2.738e-05, eta: 1:28:22, time: 0.261, data_time: 0.005, memory: 5537, decode.loss_ce: 0.2326, decode.acc_seg: 84.9926, loss: 0.2326
2022-11-29 18:15:22,204 - mmseg - INFO - Iter [21800/40000]	lr: 2.730e-05, eta: 1:28:07, time: 0.261, data_time: 0.005, memory: 5537, decode.loss_ce: 0.2624, decode.acc_seg: 86.1234, loss: 0.2624
2022-11-29 18:15:35,301 - mmseg - INFO - Iter [21850/40000]	lr: 2.723e-05, eta: 1:27:51, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2429, decode.acc_seg: 82.9379, loss: 0.2429
2022-11-29 18:15:48,346 - mmseg - INFO - Iter [21900/40000]	lr: 2.715e-05, eta: 1:27:35, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3091, decode.acc_seg: 78.6326, loss: 0.3091
2022-11-29 18:16:01,386 - mmseg - INFO - Iter [21950/40000]	lr: 2.708e-05, eta: 1:27:19, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2797, decode.acc_seg: 79.5966, loss: 0.2797
2022-11-29 18:16:14,446 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:16:14,447 - mmseg - INFO - Iter [22000/40000]	lr: 2.700e-05, eta: 1:27:04, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2838, decode.acc_seg: 81.6619, loss: 0.2838
2022-11-29 18:16:27,586 - mmseg - INFO - Iter [22050/40000]	lr: 2.693e-05, eta: 1:26:48, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2874, decode.acc_seg: 75.8713, loss: 0.2874
2022-11-29 18:16:40,721 - mmseg - INFO - Iter [22100/40000]	lr: 2.685e-05, eta: 1:26:32, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2539, decode.acc_seg: 81.2242, loss: 0.2539
2022-11-29 18:16:53,785 - mmseg - INFO - Iter [22150/40000]	lr: 2.678e-05, eta: 1:26:17, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2332, decode.acc_seg: 81.6966, loss: 0.2332
2022-11-29 18:17:06,954 - mmseg - INFO - Iter [22200/40000]	lr: 2.670e-05, eta: 1:26:01, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2446, decode.acc_seg: 82.1689, loss: 0.2446
2022-11-29 18:17:20,050 - mmseg - INFO - Iter [22250/40000]	lr: 2.663e-05, eta: 1:25:46, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3321, decode.acc_seg: 80.0265, loss: 0.3321
2022-11-29 18:17:33,106 - mmseg - INFO - Iter [22300/40000]	lr: 2.655e-05, eta: 1:25:30, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2158, decode.acc_seg: 84.5137, loss: 0.2158
2022-11-29 18:17:46,158 - mmseg - INFO - Iter [22350/40000]	lr: 2.648e-05, eta: 1:25:14, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2413, decode.acc_seg: 83.9558, loss: 0.2413
2022-11-29 18:18:01,315 - mmseg - INFO - Iter [22400/40000]	lr: 2.640e-05, eta: 1:25:00, time: 0.303, data_time: 0.047, memory: 5537, decode.loss_ce: 0.2665, decode.acc_seg: 83.5443, loss: 0.2665
2022-11-29 18:18:14,393 - mmseg - INFO - Iter [22450/40000]	lr: 2.633e-05, eta: 1:24:45, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2684, decode.acc_seg: 82.2435, loss: 0.2684
2022-11-29 18:18:27,475 - mmseg - INFO - Iter [22500/40000]	lr: 2.625e-05, eta: 1:24:29, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2491, decode.acc_seg: 84.9791, loss: 0.2491
2022-11-29 18:18:40,608 - mmseg - INFO - Iter [22550/40000]	lr: 2.618e-05, eta: 1:24:14, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3799, decode.acc_seg: 73.5355, loss: 0.3799
2022-11-29 18:18:53,769 - mmseg - INFO - Iter [22600/40000]	lr: 2.610e-05, eta: 1:23:58, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2531, decode.acc_seg: 85.0990, loss: 0.2531
2022-11-29 18:19:06,844 - mmseg - INFO - Iter [22650/40000]	lr: 2.603e-05, eta: 1:23:43, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2595, decode.acc_seg: 87.1806, loss: 0.2595
2022-11-29 18:19:20,582 - mmseg - INFO - Iter [22700/40000]	lr: 2.595e-05, eta: 1:23:28, time: 0.275, data_time: 0.019, memory: 5537, decode.loss_ce: 0.2827, decode.acc_seg: 84.0602, loss: 0.2827
2022-11-29 18:19:35,653 - mmseg - INFO - Iter [22750/40000]	lr: 2.588e-05, eta: 1:23:14, time: 0.301, data_time: 0.044, memory: 5537, decode.loss_ce: 0.1879, decode.acc_seg: 89.0850, loss: 0.1879
2022-11-29 18:19:49,169 - mmseg - INFO - Iter [22800/40000]	lr: 2.580e-05, eta: 1:22:58, time: 0.270, data_time: 0.015, memory: 5537, decode.loss_ce: 0.2491, decode.acc_seg: 83.8448, loss: 0.2491
2022-11-29 18:20:02,274 - mmseg - INFO - Iter [22850/40000]	lr: 2.573e-05, eta: 1:22:43, time: 0.262, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1763, decode.acc_seg: 88.5163, loss: 0.1763
2022-11-29 18:20:16,610 - mmseg - INFO - Iter [22900/40000]	lr: 2.565e-05, eta: 1:22:28, time: 0.287, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2181, decode.acc_seg: 85.0411, loss: 0.2181
2022-11-29 18:20:29,835 - mmseg - INFO - Iter [22950/40000]	lr: 2.558e-05, eta: 1:22:13, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2677, decode.acc_seg: 83.6029, loss: 0.2677
2022-11-29 18:20:43,376 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:20:43,377 - mmseg - INFO - Iter [23000/40000]	lr: 2.550e-05, eta: 1:21:58, time: 0.271, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3203, decode.acc_seg: 81.6293, loss: 0.3203
2022-11-29 18:20:57,136 - mmseg - INFO - Iter [23050/40000]	lr: 2.543e-05, eta: 1:21:43, time: 0.275, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2397, decode.acc_seg: 83.6759, loss: 0.2397
2022-11-29 18:21:10,347 - mmseg - INFO - Iter [23100/40000]	lr: 2.535e-05, eta: 1:21:27, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3136, decode.acc_seg: 78.0899, loss: 0.3136
2022-11-29 18:21:23,383 - mmseg - INFO - Iter [23150/40000]	lr: 2.528e-05, eta: 1:21:12, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2298, decode.acc_seg: 84.7256, loss: 0.2298
2022-11-29 18:21:36,428 - mmseg - INFO - Iter [23200/40000]	lr: 2.520e-05, eta: 1:20:56, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2379, decode.acc_seg: 83.5182, loss: 0.2379
2022-11-29 18:21:49,518 - mmseg - INFO - Iter [23250/40000]	lr: 2.513e-05, eta: 1:20:41, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3109, decode.acc_seg: 81.2366, loss: 0.3109
2022-11-29 18:22:02,595 - mmseg - INFO - Iter [23300/40000]	lr: 2.505e-05, eta: 1:20:26, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1786, decode.acc_seg: 86.4297, loss: 0.1786
2022-11-29 18:22:17,715 - mmseg - INFO - Iter [23350/40000]	lr: 2.498e-05, eta: 1:20:12, time: 0.302, data_time: 0.047, memory: 5537, decode.loss_ce: 0.2673, decode.acc_seg: 84.6932, loss: 0.2673
2022-11-29 18:22:30,859 - mmseg - INFO - Iter [23400/40000]	lr: 2.490e-05, eta: 1:19:56, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2698, decode.acc_seg: 76.2605, loss: 0.2698
2022-11-29 18:22:44,006 - mmseg - INFO - Iter [23450/40000]	lr: 2.483e-05, eta: 1:19:41, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2529, decode.acc_seg: 82.3432, loss: 0.2529
2022-11-29 18:22:57,092 - mmseg - INFO - Iter [23500/40000]	lr: 2.475e-05, eta: 1:19:25, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2268, decode.acc_seg: 85.1269, loss: 0.2268
2022-11-29 18:23:10,219 - mmseg - INFO - Iter [23550/40000]	lr: 2.468e-05, eta: 1:19:10, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2013, decode.acc_seg: 85.1222, loss: 0.2013
2022-11-29 18:23:23,313 - mmseg - INFO - Iter [23600/40000]	lr: 2.460e-05, eta: 1:18:55, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1989, decode.acc_seg: 86.1125, loss: 0.1989
2022-11-29 18:23:36,468 - mmseg - INFO - Iter [23650/40000]	lr: 2.453e-05, eta: 1:18:39, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2488, decode.acc_seg: 82.5597, loss: 0.2488
2022-11-29 18:23:49,499 - mmseg - INFO - Iter [23700/40000]	lr: 2.445e-05, eta: 1:18:24, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2043, decode.acc_seg: 86.5437, loss: 0.2043
2022-11-29 18:24:02,503 - mmseg - INFO - Iter [23750/40000]	lr: 2.438e-05, eta: 1:18:09, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1641, decode.acc_seg: 89.9825, loss: 0.1641
2022-11-29 18:24:15,624 - mmseg - INFO - Iter [23800/40000]	lr: 2.430e-05, eta: 1:17:53, time: 0.262, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1607, decode.acc_seg: 89.6060, loss: 0.1607
2022-11-29 18:24:28,638 - mmseg - INFO - Iter [23850/40000]	lr: 2.423e-05, eta: 1:17:38, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2378, decode.acc_seg: 87.1065, loss: 0.2378
2022-11-29 18:24:41,778 - mmseg - INFO - Iter [23900/40000]	lr: 2.415e-05, eta: 1:17:23, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2157, decode.acc_seg: 89.1270, loss: 0.2157
2022-11-29 18:24:54,886 - mmseg - INFO - Iter [23950/40000]	lr: 2.408e-05, eta: 1:17:07, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2348, decode.acc_seg: 86.0220, loss: 0.2348
2022-11-29 18:25:07,976 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:25:07,976 - mmseg - INFO - Iter [24000/40000]	lr: 2.400e-05, eta: 1:16:52, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2761, decode.acc_seg: 82.0093, loss: 0.2761
2022-11-29 18:25:21,096 - mmseg - INFO - Iter [24050/40000]	lr: 2.393e-05, eta: 1:16:37, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2995, decode.acc_seg: 80.5902, loss: 0.2995
2022-11-29 18:25:34,143 - mmseg - INFO - Iter [24100/40000]	lr: 2.385e-05, eta: 1:16:21, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2386, decode.acc_seg: 83.9625, loss: 0.2386
2022-11-29 18:25:47,234 - mmseg - INFO - Iter [24150/40000]	lr: 2.378e-05, eta: 1:16:06, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2125, decode.acc_seg: 84.6257, loss: 0.2125
2022-11-29 18:26:00,318 - mmseg - INFO - Iter [24200/40000]	lr: 2.370e-05, eta: 1:15:51, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2924, decode.acc_seg: 85.1320, loss: 0.2924
2022-11-29 18:26:13,505 - mmseg - INFO - Iter [24250/40000]	lr: 2.363e-05, eta: 1:15:36, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2533, decode.acc_seg: 80.7566, loss: 0.2533
2022-11-29 18:26:28,676 - mmseg - INFO - Iter [24300/40000]	lr: 2.355e-05, eta: 1:15:22, time: 0.303, data_time: 0.048, memory: 5537, decode.loss_ce: 0.2638, decode.acc_seg: 84.5794, loss: 0.2638
2022-11-29 18:26:41,760 - mmseg - INFO - Iter [24350/40000]	lr: 2.348e-05, eta: 1:15:06, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2695, decode.acc_seg: 81.7051, loss: 0.2695
2022-11-29 18:26:54,784 - mmseg - INFO - Iter [24400/40000]	lr: 2.340e-05, eta: 1:14:51, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2177, decode.acc_seg: 89.2085, loss: 0.2177
2022-11-29 18:27:07,810 - mmseg - INFO - Iter [24450/40000]	lr: 2.333e-05, eta: 1:14:36, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2714, decode.acc_seg: 84.0721, loss: 0.2714
2022-11-29 18:27:20,902 - mmseg - INFO - Iter [24500/40000]	lr: 2.325e-05, eta: 1:14:21, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3112, decode.acc_seg: 80.3151, loss: 0.3112
2022-11-29 18:27:33,965 - mmseg - INFO - Iter [24550/40000]	lr: 2.318e-05, eta: 1:14:06, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3359, decode.acc_seg: 76.8451, loss: 0.3359
2022-11-29 18:27:46,985 - mmseg - INFO - Iter [24600/40000]	lr: 2.310e-05, eta: 1:13:50, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1921, decode.acc_seg: 89.0429, loss: 0.1921
2022-11-29 18:28:00,018 - mmseg - INFO - Iter [24650/40000]	lr: 2.303e-05, eta: 1:13:35, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2632, decode.acc_seg: 82.8371, loss: 0.2632
2022-11-29 18:28:13,096 - mmseg - INFO - Iter [24700/40000]	lr: 2.295e-05, eta: 1:13:20, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2386, decode.acc_seg: 86.6001, loss: 0.2386
2022-11-29 18:28:26,376 - mmseg - INFO - Iter [24750/40000]	lr: 2.288e-05, eta: 1:13:05, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2198, decode.acc_seg: 85.7756, loss: 0.2198
2022-11-29 18:28:39,497 - mmseg - INFO - Iter [24800/40000]	lr: 2.280e-05, eta: 1:12:50, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2156, decode.acc_seg: 85.8493, loss: 0.2156
2022-11-29 18:28:52,606 - mmseg - INFO - Iter [24850/40000]	lr: 2.273e-05, eta: 1:12:34, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1933, decode.acc_seg: 86.9663, loss: 0.1933
2022-11-29 18:29:05,750 - mmseg - INFO - Iter [24900/40000]	lr: 2.265e-05, eta: 1:12:19, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2476, decode.acc_seg: 84.3528, loss: 0.2476
2022-11-29 18:29:18,835 - mmseg - INFO - Iter [24950/40000]	lr: 2.258e-05, eta: 1:12:04, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3365, decode.acc_seg: 79.7331, loss: 0.3365
2022-11-29 18:29:31,955 - mmseg - INFO - Saving checkpoint at 25000 iterations
2022-11-29 18:29:34,769 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:29:34,770 - mmseg - INFO - Iter [25000/40000]	lr: 2.250e-05, eta: 1:11:51, time: 0.319, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2334, decode.acc_seg: 84.3683, loss: 0.2334
2022-11-29 18:31:22,490 - mmseg - INFO - per class results:
2022-11-29 18:31:22,491 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
| background | 60.08 | 70.77 |
|  building  |  9.96 | 10.26 |
|  woodland  | 42.99 | 46.76 |
|   water    | 11.92 | 62.22 |
|    road    |  9.83 | 11.78 |
+------------+-------+-------+
2022-11-29 18:31:22,491 - mmseg - INFO - Summary:
2022-11-29 18:31:22,491 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 61.22 | 26.95 | 40.36 |
+-------+-------+-------+
2022-11-29 18:31:22,495 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:31:22,495 - mmseg - INFO - Iter(val) [1602]	aAcc: 0.6122, mIoU: 0.2695, mAcc: 0.4036, IoU.background: 0.6008, IoU.building: 0.0996, IoU.woodland: 0.4299, IoU.water: 0.1192, IoU.road: 0.0983, Acc.background: 0.7077, Acc.building: 0.1026, Acc.woodland: 0.4676, Acc.water: 0.6222, Acc.road: 0.1178
2022-11-29 18:31:36,012 - mmseg - INFO - Iter [25050/40000]	lr: 2.243e-05, eta: 1:12:40, time: 2.425, data_time: 2.161, memory: 5537, decode.loss_ce: 0.2151, decode.acc_seg: 86.7240, loss: 0.2151
2022-11-29 18:31:49,057 - mmseg - INFO - Iter [25100/40000]	lr: 2.235e-05, eta: 1:12:25, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2329, decode.acc_seg: 81.8046, loss: 0.2329
2022-11-29 18:32:02,050 - mmseg - INFO - Iter [25150/40000]	lr: 2.228e-05, eta: 1:12:09, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2382, decode.acc_seg: 85.4719, loss: 0.2382
2022-11-29 18:32:17,243 - mmseg - INFO - Iter [25200/40000]	lr: 2.220e-05, eta: 1:11:55, time: 0.304, data_time: 0.047, memory: 5537, decode.loss_ce: 0.2471, decode.acc_seg: 86.0804, loss: 0.2471
2022-11-29 18:32:30,254 - mmseg - INFO - Iter [25250/40000]	lr: 2.213e-05, eta: 1:11:39, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1607, decode.acc_seg: 87.6114, loss: 0.1607
2022-11-29 18:32:43,281 - mmseg - INFO - Iter [25300/40000]	lr: 2.205e-05, eta: 1:11:24, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2537, decode.acc_seg: 82.2355, loss: 0.2537
2022-11-29 18:32:57,498 - mmseg - INFO - Iter [25350/40000]	lr: 2.198e-05, eta: 1:11:09, time: 0.284, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2595, decode.acc_seg: 82.9819, loss: 0.2595
2022-11-29 18:33:11,672 - mmseg - INFO - Iter [25400/40000]	lr: 2.190e-05, eta: 1:10:54, time: 0.283, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2244, decode.acc_seg: 83.5222, loss: 0.2244
2022-11-29 18:33:24,725 - mmseg - INFO - Iter [25450/40000]	lr: 2.183e-05, eta: 1:10:39, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2364, decode.acc_seg: 86.2642, loss: 0.2364
2022-11-29 18:33:38,402 - mmseg - INFO - Iter [25500/40000]	lr: 2.175e-05, eta: 1:10:24, time: 0.274, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2487, decode.acc_seg: 85.1435, loss: 0.2487
2022-11-29 18:33:54,210 - mmseg - INFO - Iter [25550/40000]	lr: 2.168e-05, eta: 1:10:10, time: 0.316, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1940, decode.acc_seg: 87.7624, loss: 0.1940
2022-11-29 18:34:09,886 - mmseg - INFO - Iter [25600/40000]	lr: 2.160e-05, eta: 1:09:56, time: 0.313, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2151, decode.acc_seg: 88.6842, loss: 0.2151
2022-11-29 18:34:25,594 - mmseg - INFO - Iter [25650/40000]	lr: 2.153e-05, eta: 1:09:42, time: 0.314, data_time: 0.007, memory: 5537, decode.loss_ce: 0.3217, decode.acc_seg: 78.2961, loss: 0.3217
2022-11-29 18:34:41,302 - mmseg - INFO - Iter [25700/40000]	lr: 2.145e-05, eta: 1:09:28, time: 0.314, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2212, decode.acc_seg: 86.8317, loss: 0.2212
2022-11-29 18:34:54,703 - mmseg - INFO - Iter [25750/40000]	lr: 2.138e-05, eta: 1:09:13, time: 0.268, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2598, decode.acc_seg: 83.1298, loss: 0.2598
2022-11-29 18:35:07,778 - mmseg - INFO - Iter [25800/40000]	lr: 2.130e-05, eta: 1:08:58, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2278, decode.acc_seg: 81.7829, loss: 0.2278
2022-11-29 18:35:20,915 - mmseg - INFO - Iter [25850/40000]	lr: 2.123e-05, eta: 1:08:42, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2137, decode.acc_seg: 85.0508, loss: 0.2137
2022-11-29 18:35:33,983 - mmseg - INFO - Iter [25900/40000]	lr: 2.115e-05, eta: 1:08:27, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2635, decode.acc_seg: 80.9119, loss: 0.2635
2022-11-29 18:35:47,049 - mmseg - INFO - Iter [25950/40000]	lr: 2.108e-05, eta: 1:08:11, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1601, decode.acc_seg: 89.4016, loss: 0.1601
2022-11-29 18:36:00,163 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:36:00,163 - mmseg - INFO - Iter [26000/40000]	lr: 2.100e-05, eta: 1:07:56, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2831, decode.acc_seg: 86.3229, loss: 0.2831
2022-11-29 18:36:13,226 - mmseg - INFO - Iter [26050/40000]	lr: 2.093e-05, eta: 1:07:41, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2358, decode.acc_seg: 86.0071, loss: 0.2358
2022-11-29 18:36:26,305 - mmseg - INFO - Iter [26100/40000]	lr: 2.085e-05, eta: 1:07:25, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3192, decode.acc_seg: 82.5031, loss: 0.3192
2022-11-29 18:36:41,625 - mmseg - INFO - Iter [26150/40000]	lr: 2.078e-05, eta: 1:07:11, time: 0.306, data_time: 0.048, memory: 5537, decode.loss_ce: 0.1947, decode.acc_seg: 86.7184, loss: 0.1947
2022-11-29 18:36:54,714 - mmseg - INFO - Iter [26200/40000]	lr: 2.070e-05, eta: 1:06:56, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2967, decode.acc_seg: 83.0950, loss: 0.2967
2022-11-29 18:37:07,794 - mmseg - INFO - Iter [26250/40000]	lr: 2.063e-05, eta: 1:06:41, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2036, decode.acc_seg: 87.0254, loss: 0.2036
2022-11-29 18:37:20,947 - mmseg - INFO - Iter [26300/40000]	lr: 2.055e-05, eta: 1:06:25, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2632, decode.acc_seg: 85.4655, loss: 0.2632
2022-11-29 18:37:34,065 - mmseg - INFO - Iter [26350/40000]	lr: 2.048e-05, eta: 1:06:10, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1558, decode.acc_seg: 89.4321, loss: 0.1558
2022-11-29 18:37:47,265 - mmseg - INFO - Iter [26400/40000]	lr: 2.040e-05, eta: 1:05:55, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2727, decode.acc_seg: 85.2722, loss: 0.2727
2022-11-29 18:38:00,331 - mmseg - INFO - Iter [26450/40000]	lr: 2.033e-05, eta: 1:05:40, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2292, decode.acc_seg: 86.1111, loss: 0.2292
2022-11-29 18:38:13,527 - mmseg - INFO - Iter [26500/40000]	lr: 2.025e-05, eta: 1:05:24, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1537, decode.acc_seg: 91.6442, loss: 0.1537
2022-11-29 18:38:26,650 - mmseg - INFO - Iter [26550/40000]	lr: 2.018e-05, eta: 1:05:09, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1956, decode.acc_seg: 88.4739, loss: 0.1956
2022-11-29 18:38:39,772 - mmseg - INFO - Iter [26600/40000]	lr: 2.010e-05, eta: 1:04:54, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2109, decode.acc_seg: 88.0884, loss: 0.2109
2022-11-29 18:38:52,880 - mmseg - INFO - Iter [26650/40000]	lr: 2.003e-05, eta: 1:04:39, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2139, decode.acc_seg: 87.2045, loss: 0.2139
2022-11-29 18:39:05,985 - mmseg - INFO - Iter [26700/40000]	lr: 1.995e-05, eta: 1:04:23, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2367, decode.acc_seg: 85.7335, loss: 0.2367
2022-11-29 18:39:19,064 - mmseg - INFO - Iter [26750/40000]	lr: 1.988e-05, eta: 1:04:08, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2348, decode.acc_seg: 88.2274, loss: 0.2348
2022-11-29 18:39:32,112 - mmseg - INFO - Iter [26800/40000]	lr: 1.980e-05, eta: 1:03:53, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2160, decode.acc_seg: 85.1765, loss: 0.2160
2022-11-29 18:39:45,243 - mmseg - INFO - Iter [26850/40000]	lr: 1.973e-05, eta: 1:03:38, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2581, decode.acc_seg: 85.5764, loss: 0.2581
2022-11-29 18:39:58,393 - mmseg - INFO - Iter [26900/40000]	lr: 1.965e-05, eta: 1:03:22, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2135, decode.acc_seg: 86.4529, loss: 0.2135
2022-11-29 18:40:11,465 - mmseg - INFO - Iter [26950/40000]	lr: 1.958e-05, eta: 1:03:07, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2735, decode.acc_seg: 84.7837, loss: 0.2735
2022-11-29 18:40:24,586 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:40:24,586 - mmseg - INFO - Iter [27000/40000]	lr: 1.950e-05, eta: 1:02:52, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2342, decode.acc_seg: 86.9477, loss: 0.2342
2022-11-29 18:40:37,723 - mmseg - INFO - Iter [27050/40000]	lr: 1.943e-05, eta: 1:02:37, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2289, decode.acc_seg: 81.0195, loss: 0.2289
2022-11-29 18:40:52,895 - mmseg - INFO - Iter [27100/40000]	lr: 1.935e-05, eta: 1:02:23, time: 0.303, data_time: 0.047, memory: 5537, decode.loss_ce: 0.2526, decode.acc_seg: 82.2500, loss: 0.2526
2022-11-29 18:41:05,905 - mmseg - INFO - Iter [27150/40000]	lr: 1.928e-05, eta: 1:02:07, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1931, decode.acc_seg: 87.6002, loss: 0.1931
2022-11-29 18:41:19,017 - mmseg - INFO - Iter [27200/40000]	lr: 1.920e-05, eta: 1:01:52, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2310, decode.acc_seg: 85.2398, loss: 0.2310
2022-11-29 18:41:32,107 - mmseg - INFO - Iter [27250/40000]	lr: 1.913e-05, eta: 1:01:37, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2500, decode.acc_seg: 83.8279, loss: 0.2500
2022-11-29 18:41:45,151 - mmseg - INFO - Iter [27300/40000]	lr: 1.905e-05, eta: 1:01:22, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1842, decode.acc_seg: 89.2586, loss: 0.1842
2022-11-29 18:41:58,203 - mmseg - INFO - Iter [27350/40000]	lr: 1.898e-05, eta: 1:01:07, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3136, decode.acc_seg: 80.7168, loss: 0.3136
2022-11-29 18:42:11,297 - mmseg - INFO - Iter [27400/40000]	lr: 1.890e-05, eta: 1:00:52, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2060, decode.acc_seg: 89.6796, loss: 0.2060
2022-11-29 18:42:24,448 - mmseg - INFO - Iter [27450/40000]	lr: 1.883e-05, eta: 1:00:37, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1920, decode.acc_seg: 84.0796, loss: 0.1920
2022-11-29 18:42:37,503 - mmseg - INFO - Iter [27500/40000]	lr: 1.875e-05, eta: 1:00:21, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2258, decode.acc_seg: 86.3409, loss: 0.2258
2022-11-29 18:42:50,670 - mmseg - INFO - Iter [27550/40000]	lr: 1.868e-05, eta: 1:00:06, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2161, decode.acc_seg: 87.9930, loss: 0.2161
2022-11-29 18:43:03,737 - mmseg - INFO - Iter [27600/40000]	lr: 1.860e-05, eta: 0:59:51, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1913, decode.acc_seg: 88.1206, loss: 0.1913
2022-11-29 18:43:16,763 - mmseg - INFO - Iter [27650/40000]	lr: 1.853e-05, eta: 0:59:36, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2425, decode.acc_seg: 85.2709, loss: 0.2425
2022-11-29 18:43:29,887 - mmseg - INFO - Iter [27700/40000]	lr: 1.845e-05, eta: 0:59:21, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2864, decode.acc_seg: 83.1860, loss: 0.2864
2022-11-29 18:43:43,040 - mmseg - INFO - Iter [27750/40000]	lr: 1.838e-05, eta: 0:59:06, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2240, decode.acc_seg: 84.8972, loss: 0.2240
2022-11-29 18:43:56,197 - mmseg - INFO - Iter [27800/40000]	lr: 1.830e-05, eta: 0:58:51, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1630, decode.acc_seg: 90.2720, loss: 0.1630
2022-11-29 18:44:09,295 - mmseg - INFO - Iter [27850/40000]	lr: 1.823e-05, eta: 0:58:36, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2320, decode.acc_seg: 84.4365, loss: 0.2320
2022-11-29 18:44:23,193 - mmseg - INFO - Iter [27900/40000]	lr: 1.815e-05, eta: 0:58:21, time: 0.278, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2568, decode.acc_seg: 80.9726, loss: 0.2568
2022-11-29 18:44:36,299 - mmseg - INFO - Iter [27950/40000]	lr: 1.808e-05, eta: 0:58:06, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2491, decode.acc_seg: 85.3674, loss: 0.2491
2022-11-29 18:44:51,695 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:44:51,695 - mmseg - INFO - Iter [28000/40000]	lr: 1.800e-05, eta: 0:57:52, time: 0.308, data_time: 0.048, memory: 5537, decode.loss_ce: 0.2082, decode.acc_seg: 86.2058, loss: 0.2082
2022-11-29 18:45:04,702 - mmseg - INFO - Iter [28050/40000]	lr: 1.793e-05, eta: 0:57:37, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1782, decode.acc_seg: 89.5699, loss: 0.1782
2022-11-29 18:45:17,714 - mmseg - INFO - Iter [28100/40000]	lr: 1.785e-05, eta: 0:57:22, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2024, decode.acc_seg: 85.6582, loss: 0.2024
2022-11-29 18:45:30,870 - mmseg - INFO - Iter [28150/40000]	lr: 1.778e-05, eta: 0:57:07, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1982, decode.acc_seg: 86.4791, loss: 0.1982
2022-11-29 18:45:43,887 - mmseg - INFO - Iter [28200/40000]	lr: 1.770e-05, eta: 0:56:52, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1815, decode.acc_seg: 89.0239, loss: 0.1815
2022-11-29 18:45:56,906 - mmseg - INFO - Iter [28250/40000]	lr: 1.763e-05, eta: 0:56:37, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2090, decode.acc_seg: 86.6261, loss: 0.2090
2022-11-29 18:46:09,972 - mmseg - INFO - Iter [28300/40000]	lr: 1.755e-05, eta: 0:56:22, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1889, decode.acc_seg: 87.2646, loss: 0.1889
2022-11-29 18:46:23,038 - mmseg - INFO - Iter [28350/40000]	lr: 1.748e-05, eta: 0:56:07, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1804, decode.acc_seg: 89.3361, loss: 0.1804
2022-11-29 18:46:36,125 - mmseg - INFO - Iter [28400/40000]	lr: 1.740e-05, eta: 0:55:52, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2201, decode.acc_seg: 85.6053, loss: 0.2201
2022-11-29 18:46:49,143 - mmseg - INFO - Iter [28450/40000]	lr: 1.733e-05, eta: 0:55:37, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2159, decode.acc_seg: 88.5877, loss: 0.2159
2022-11-29 18:47:02,252 - mmseg - INFO - Iter [28500/40000]	lr: 1.725e-05, eta: 0:55:22, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2735, decode.acc_seg: 83.6371, loss: 0.2735
2022-11-29 18:47:16,096 - mmseg - INFO - Iter [28550/40000]	lr: 1.718e-05, eta: 0:55:07, time: 0.277, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2456, decode.acc_seg: 83.9649, loss: 0.2456
2022-11-29 18:47:30,729 - mmseg - INFO - Iter [28600/40000]	lr: 1.710e-05, eta: 0:54:53, time: 0.293, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2959, decode.acc_seg: 82.1001, loss: 0.2959
2022-11-29 18:47:43,835 - mmseg - INFO - Iter [28650/40000]	lr: 1.703e-05, eta: 0:54:38, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2589, decode.acc_seg: 82.6631, loss: 0.2589
2022-11-29 18:47:56,998 - mmseg - INFO - Iter [28700/40000]	lr: 1.695e-05, eta: 0:54:23, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1771, decode.acc_seg: 89.2561, loss: 0.1771
2022-11-29 18:48:10,044 - mmseg - INFO - Iter [28750/40000]	lr: 1.688e-05, eta: 0:54:08, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1973, decode.acc_seg: 89.5126, loss: 0.1973
2022-11-29 18:48:23,036 - mmseg - INFO - Iter [28800/40000]	lr: 1.680e-05, eta: 0:53:53, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2000, decode.acc_seg: 86.6309, loss: 0.2000
2022-11-29 18:48:36,097 - mmseg - INFO - Iter [28850/40000]	lr: 1.673e-05, eta: 0:53:38, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1930, decode.acc_seg: 86.8005, loss: 0.1930
2022-11-29 18:48:49,150 - mmseg - INFO - Iter [28900/40000]	lr: 1.665e-05, eta: 0:53:23, time: 0.261, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2045, decode.acc_seg: 84.7078, loss: 0.2045
2022-11-29 18:49:04,302 - mmseg - INFO - Iter [28950/40000]	lr: 1.658e-05, eta: 0:53:09, time: 0.303, data_time: 0.047, memory: 5537, decode.loss_ce: 0.1885, decode.acc_seg: 87.8774, loss: 0.1885
2022-11-29 18:49:17,361 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:49:17,361 - mmseg - INFO - Iter [29000/40000]	lr: 1.650e-05, eta: 0:52:54, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2473, decode.acc_seg: 82.2797, loss: 0.2473
2022-11-29 18:49:30,489 - mmseg - INFO - Iter [29050/40000]	lr: 1.643e-05, eta: 0:52:39, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2360, decode.acc_seg: 81.8602, loss: 0.2360
2022-11-29 18:49:43,591 - mmseg - INFO - Iter [29100/40000]	lr: 1.635e-05, eta: 0:52:24, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1943, decode.acc_seg: 87.7407, loss: 0.1943
2022-11-29 18:49:56,638 - mmseg - INFO - Iter [29150/40000]	lr: 1.628e-05, eta: 0:52:09, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3772, decode.acc_seg: 80.0581, loss: 0.3772
2022-11-29 18:50:09,806 - mmseg - INFO - Iter [29200/40000]	lr: 1.620e-05, eta: 0:51:54, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2302, decode.acc_seg: 83.7612, loss: 0.2302
2022-11-29 18:50:22,875 - mmseg - INFO - Iter [29250/40000]	lr: 1.613e-05, eta: 0:51:39, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2107, decode.acc_seg: 86.5742, loss: 0.2107
2022-11-29 18:50:37,957 - mmseg - INFO - Iter [29300/40000]	lr: 1.605e-05, eta: 0:51:25, time: 0.302, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1892, decode.acc_seg: 87.7734, loss: 0.1892
2022-11-29 18:50:53,684 - mmseg - INFO - Iter [29350/40000]	lr: 1.598e-05, eta: 0:51:11, time: 0.315, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1823, decode.acc_seg: 87.7769, loss: 0.1823
2022-11-29 18:51:08,185 - mmseg - INFO - Iter [29400/40000]	lr: 1.590e-05, eta: 0:50:56, time: 0.290, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1636, decode.acc_seg: 88.5346, loss: 0.1636
2022-11-29 18:51:21,241 - mmseg - INFO - Iter [29450/40000]	lr: 1.583e-05, eta: 0:50:42, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2459, decode.acc_seg: 85.1818, loss: 0.2459
2022-11-29 18:51:34,385 - mmseg - INFO - Iter [29500/40000]	lr: 1.575e-05, eta: 0:50:27, time: 0.263, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2072, decode.acc_seg: 87.9917, loss: 0.2072
2022-11-29 18:51:47,486 - mmseg - INFO - Iter [29550/40000]	lr: 1.568e-05, eta: 0:50:12, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2631, decode.acc_seg: 83.1057, loss: 0.2631
2022-11-29 18:52:00,704 - mmseg - INFO - Iter [29600/40000]	lr: 1.560e-05, eta: 0:49:57, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1844, decode.acc_seg: 88.2276, loss: 0.1844
2022-11-29 18:52:13,732 - mmseg - INFO - Iter [29650/40000]	lr: 1.553e-05, eta: 0:49:42, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1709, decode.acc_seg: 85.9759, loss: 0.1709
2022-11-29 18:52:26,847 - mmseg - INFO - Iter [29700/40000]	lr: 1.545e-05, eta: 0:49:27, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1817, decode.acc_seg: 89.1878, loss: 0.1817
2022-11-29 18:52:39,941 - mmseg - INFO - Iter [29750/40000]	lr: 1.538e-05, eta: 0:49:12, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1851, decode.acc_seg: 85.9567, loss: 0.1851
2022-11-29 18:52:52,936 - mmseg - INFO - Iter [29800/40000]	lr: 1.530e-05, eta: 0:48:58, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1872, decode.acc_seg: 86.6762, loss: 0.1872
2022-11-29 18:53:06,062 - mmseg - INFO - Iter [29850/40000]	lr: 1.523e-05, eta: 0:48:43, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2151, decode.acc_seg: 86.9824, loss: 0.2151
2022-11-29 18:53:21,670 - mmseg - INFO - Iter [29900/40000]	lr: 1.515e-05, eta: 0:48:29, time: 0.312, data_time: 0.054, memory: 5537, decode.loss_ce: 0.1747, decode.acc_seg: 89.1896, loss: 0.1747
2022-11-29 18:53:35,157 - mmseg - INFO - Iter [29950/40000]	lr: 1.508e-05, eta: 0:48:14, time: 0.270, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1646, decode.acc_seg: 87.8158, loss: 0.1646
2022-11-29 18:53:50,876 - mmseg - INFO - Saving checkpoint at 30000 iterations
2022-11-29 18:53:52,585 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:53:52,585 - mmseg - INFO - Iter [30000/40000]	lr: 1.500e-05, eta: 0:48:01, time: 0.349, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2100, decode.acc_seg: 85.9025, loss: 0.2100
2022-11-29 18:55:16,218 - mmseg - INFO - per class results:
2022-11-29 18:55:16,218 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
| background | 59.72 | 74.01 |
|  building  | 10.28 | 10.71 |
|  woodland  | 40.28 | 43.23 |
|   water    | 16.07 | 71.48 |
|    road    |  9.14 | 10.83 |
+------------+-------+-------+
2022-11-29 18:55:16,218 - mmseg - INFO - Summary:
2022-11-29 18:55:16,219 - mmseg - INFO - 
+-------+------+-------+
|  aAcc | mIoU |  mAcc |
+-------+------+-------+
| 62.59 | 27.1 | 42.05 |
+-------+------+-------+
2022-11-29 18:55:16,221 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:55:16,221 - mmseg - INFO - Iter(val) [1602]	aAcc: 0.6259, mIoU: 0.2710, mAcc: 0.4205, IoU.background: 0.5972, IoU.building: 0.1028, IoU.woodland: 0.4028, IoU.water: 0.1607, IoU.road: 0.0914, Acc.background: 0.7401, Acc.building: 0.1071, Acc.woodland: 0.4323, Acc.water: 0.7148, Acc.road: 0.1083
2022-11-29 18:55:29,352 - mmseg - INFO - Iter [30050/40000]	lr: 1.493e-05, eta: 0:48:13, time: 1.935, data_time: 1.679, memory: 5537, decode.loss_ce: 0.1924, decode.acc_seg: 88.3827, loss: 0.1924
2022-11-29 18:55:42,421 - mmseg - INFO - Iter [30100/40000]	lr: 1.485e-05, eta: 0:47:58, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3976, decode.acc_seg: 82.7276, loss: 0.3976
2022-11-29 18:55:55,548 - mmseg - INFO - Iter [30150/40000]	lr: 1.478e-05, eta: 0:47:43, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2370, decode.acc_seg: 84.0261, loss: 0.2370
2022-11-29 18:56:08,672 - mmseg - INFO - Iter [30200/40000]	lr: 1.470e-05, eta: 0:47:28, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2170, decode.acc_seg: 85.8080, loss: 0.2170
2022-11-29 18:56:21,753 - mmseg - INFO - Iter [30250/40000]	lr: 1.463e-05, eta: 0:47:13, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1820, decode.acc_seg: 88.1408, loss: 0.1820
2022-11-29 18:56:34,926 - mmseg - INFO - Iter [30300/40000]	lr: 1.455e-05, eta: 0:46:58, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1792, decode.acc_seg: 87.7068, loss: 0.1792
2022-11-29 18:56:47,976 - mmseg - INFO - Iter [30350/40000]	lr: 1.448e-05, eta: 0:46:43, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2355, decode.acc_seg: 84.2014, loss: 0.2355
2022-11-29 18:57:01,115 - mmseg - INFO - Iter [30400/40000]	lr: 1.440e-05, eta: 0:46:28, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3066, decode.acc_seg: 82.8085, loss: 0.3066
2022-11-29 18:57:14,209 - mmseg - INFO - Iter [30450/40000]	lr: 1.433e-05, eta: 0:46:13, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2156, decode.acc_seg: 85.8336, loss: 0.2156
2022-11-29 18:57:27,309 - mmseg - INFO - Iter [30500/40000]	lr: 1.425e-05, eta: 0:45:59, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1736, decode.acc_seg: 88.0116, loss: 0.1736
2022-11-29 18:57:40,375 - mmseg - INFO - Iter [30550/40000]	lr: 1.418e-05, eta: 0:45:44, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2090, decode.acc_seg: 86.4113, loss: 0.2090
2022-11-29 18:57:53,481 - mmseg - INFO - Iter [30600/40000]	lr: 1.410e-05, eta: 0:45:29, time: 0.262, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1676, decode.acc_seg: 87.5957, loss: 0.1676
2022-11-29 18:58:06,789 - mmseg - INFO - Iter [30650/40000]	lr: 1.403e-05, eta: 0:45:14, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2229, decode.acc_seg: 83.3975, loss: 0.2229
2022-11-29 18:58:19,863 - mmseg - INFO - Iter [30700/40000]	lr: 1.395e-05, eta: 0:44:59, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1809, decode.acc_seg: 87.6263, loss: 0.1809
2022-11-29 18:58:32,987 - mmseg - INFO - Iter [30750/40000]	lr: 1.388e-05, eta: 0:44:44, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1425, decode.acc_seg: 90.6487, loss: 0.1425
2022-11-29 18:58:48,257 - mmseg - INFO - Iter [30800/40000]	lr: 1.380e-05, eta: 0:44:30, time: 0.305, data_time: 0.048, memory: 5537, decode.loss_ce: 0.1742, decode.acc_seg: 88.8180, loss: 0.1742
2022-11-29 18:59:01,319 - mmseg - INFO - Iter [30850/40000]	lr: 1.373e-05, eta: 0:44:15, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1950, decode.acc_seg: 87.9886, loss: 0.1950
2022-11-29 18:59:14,484 - mmseg - INFO - Iter [30900/40000]	lr: 1.365e-05, eta: 0:44:00, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2159, decode.acc_seg: 83.7230, loss: 0.2159
2022-11-29 18:59:27,586 - mmseg - INFO - Iter [30950/40000]	lr: 1.358e-05, eta: 0:43:45, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2455, decode.acc_seg: 83.4513, loss: 0.2455
2022-11-29 18:59:40,671 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 18:59:40,671 - mmseg - INFO - Iter [31000/40000]	lr: 1.350e-05, eta: 0:43:30, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1935, decode.acc_seg: 88.0016, loss: 0.1935
2022-11-29 18:59:53,802 - mmseg - INFO - Iter [31050/40000]	lr: 1.343e-05, eta: 0:43:15, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2600, decode.acc_seg: 87.3673, loss: 0.2600
2022-11-29 19:00:07,344 - mmseg - INFO - Iter [31100/40000]	lr: 1.335e-05, eta: 0:43:00, time: 0.271, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1733, decode.acc_seg: 87.0577, loss: 0.1733
2022-11-29 19:00:20,468 - mmseg - INFO - Iter [31150/40000]	lr: 1.328e-05, eta: 0:42:45, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2051, decode.acc_seg: 86.4171, loss: 0.2051
2022-11-29 19:00:33,493 - mmseg - INFO - Iter [31200/40000]	lr: 1.320e-05, eta: 0:42:30, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1906, decode.acc_seg: 88.7576, loss: 0.1906
2022-11-29 19:00:46,584 - mmseg - INFO - Iter [31250/40000]	lr: 1.313e-05, eta: 0:42:16, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2442, decode.acc_seg: 83.8355, loss: 0.2442
2022-11-29 19:00:59,671 - mmseg - INFO - Iter [31300/40000]	lr: 1.305e-05, eta: 0:42:01, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2237, decode.acc_seg: 84.2754, loss: 0.2237
2022-11-29 19:01:12,792 - mmseg - INFO - Iter [31350/40000]	lr: 1.298e-05, eta: 0:41:46, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1692, decode.acc_seg: 89.0000, loss: 0.1692
2022-11-29 19:01:25,845 - mmseg - INFO - Iter [31400/40000]	lr: 1.290e-05, eta: 0:41:31, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1980, decode.acc_seg: 86.6034, loss: 0.1980
2022-11-29 19:01:38,970 - mmseg - INFO - Iter [31450/40000]	lr: 1.283e-05, eta: 0:41:16, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1826, decode.acc_seg: 87.4350, loss: 0.1826
2022-11-29 19:01:52,604 - mmseg - INFO - Iter [31500/40000]	lr: 1.275e-05, eta: 0:41:01, time: 0.273, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2411, decode.acc_seg: 86.6170, loss: 0.2411
2022-11-29 19:02:08,303 - mmseg - INFO - Iter [31550/40000]	lr: 1.268e-05, eta: 0:40:47, time: 0.314, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2027, decode.acc_seg: 85.7079, loss: 0.2027
2022-11-29 19:02:24,012 - mmseg - INFO - Iter [31600/40000]	lr: 1.260e-05, eta: 0:40:33, time: 0.314, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2273, decode.acc_seg: 83.5820, loss: 0.2273
2022-11-29 19:02:39,728 - mmseg - INFO - Iter [31650/40000]	lr: 1.253e-05, eta: 0:40:19, time: 0.314, data_time: 0.007, memory: 5537, decode.loss_ce: 0.2076, decode.acc_seg: 87.0441, loss: 0.2076
2022-11-29 19:02:52,789 - mmseg - INFO - Iter [31700/40000]	lr: 1.245e-05, eta: 0:40:04, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1923, decode.acc_seg: 84.5516, loss: 0.1923
2022-11-29 19:03:08,010 - mmseg - INFO - Iter [31750/40000]	lr: 1.238e-05, eta: 0:39:50, time: 0.304, data_time: 0.047, memory: 5537, decode.loss_ce: 0.2571, decode.acc_seg: 84.6218, loss: 0.2571
2022-11-29 19:03:21,062 - mmseg - INFO - Iter [31800/40000]	lr: 1.230e-05, eta: 0:39:35, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1604, decode.acc_seg: 89.0292, loss: 0.1604
2022-11-29 19:03:34,277 - mmseg - INFO - Iter [31850/40000]	lr: 1.223e-05, eta: 0:39:20, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1640, decode.acc_seg: 87.2345, loss: 0.1640
2022-11-29 19:03:47,372 - mmseg - INFO - Iter [31900/40000]	lr: 1.215e-05, eta: 0:39:05, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2139, decode.acc_seg: 87.7560, loss: 0.2139
2022-11-29 19:04:00,382 - mmseg - INFO - Iter [31950/40000]	lr: 1.208e-05, eta: 0:38:50, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2173, decode.acc_seg: 86.4716, loss: 0.2173
2022-11-29 19:04:13,428 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:04:13,428 - mmseg - INFO - Iter [32000/40000]	lr: 1.200e-05, eta: 0:38:36, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2418, decode.acc_seg: 84.7929, loss: 0.2418
2022-11-29 19:04:26,447 - mmseg - INFO - Iter [32050/40000]	lr: 1.193e-05, eta: 0:38:21, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2253, decode.acc_seg: 86.0471, loss: 0.2253
2022-11-29 19:04:39,553 - mmseg - INFO - Iter [32100/40000]	lr: 1.185e-05, eta: 0:38:06, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2116, decode.acc_seg: 83.7607, loss: 0.2116
2022-11-29 19:04:52,781 - mmseg - INFO - Iter [32150/40000]	lr: 1.178e-05, eta: 0:37:51, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1633, decode.acc_seg: 87.8786, loss: 0.1633
2022-11-29 19:05:05,949 - mmseg - INFO - Iter [32200/40000]	lr: 1.170e-05, eta: 0:37:36, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1639, decode.acc_seg: 88.3616, loss: 0.1639
2022-11-29 19:05:19,024 - mmseg - INFO - Iter [32250/40000]	lr: 1.163e-05, eta: 0:37:22, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2040, decode.acc_seg: 86.9000, loss: 0.2040
2022-11-29 19:05:32,098 - mmseg - INFO - Iter [32300/40000]	lr: 1.155e-05, eta: 0:37:07, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1531, decode.acc_seg: 89.0418, loss: 0.1531
2022-11-29 19:05:45,174 - mmseg - INFO - Iter [32350/40000]	lr: 1.148e-05, eta: 0:36:52, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1878, decode.acc_seg: 87.1965, loss: 0.1878
2022-11-29 19:05:58,233 - mmseg - INFO - Iter [32400/40000]	lr: 1.140e-05, eta: 0:36:37, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1431, decode.acc_seg: 89.8743, loss: 0.1431
2022-11-29 19:06:11,339 - mmseg - INFO - Iter [32450/40000]	lr: 1.133e-05, eta: 0:36:22, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2085, decode.acc_seg: 88.3527, loss: 0.2085
2022-11-29 19:06:24,413 - mmseg - INFO - Iter [32500/40000]	lr: 1.125e-05, eta: 0:36:08, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2066, decode.acc_seg: 86.8881, loss: 0.2066
2022-11-29 19:06:37,508 - mmseg - INFO - Iter [32550/40000]	lr: 1.118e-05, eta: 0:35:53, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2362, decode.acc_seg: 85.6660, loss: 0.2362
2022-11-29 19:06:50,694 - mmseg - INFO - Iter [32600/40000]	lr: 1.110e-05, eta: 0:35:38, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1836, decode.acc_seg: 88.9238, loss: 0.1836
2022-11-29 19:07:06,020 - mmseg - INFO - Iter [32650/40000]	lr: 1.103e-05, eta: 0:35:24, time: 0.306, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1708, decode.acc_seg: 87.2708, loss: 0.1708
2022-11-29 19:07:22,151 - mmseg - INFO - Iter [32700/40000]	lr: 1.095e-05, eta: 0:35:10, time: 0.323, data_time: 0.049, memory: 5537, decode.loss_ce: 0.2601, decode.acc_seg: 81.9662, loss: 0.2601
2022-11-29 19:07:35,433 - mmseg - INFO - Iter [32750/40000]	lr: 1.088e-05, eta: 0:34:55, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3678, decode.acc_seg: 74.6204, loss: 0.3678
2022-11-29 19:07:48,697 - mmseg - INFO - Iter [32800/40000]	lr: 1.080e-05, eta: 0:34:40, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2086, decode.acc_seg: 88.4524, loss: 0.2086
2022-11-29 19:08:01,698 - mmseg - INFO - Iter [32850/40000]	lr: 1.073e-05, eta: 0:34:26, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2203, decode.acc_seg: 81.6785, loss: 0.2203
2022-11-29 19:08:14,754 - mmseg - INFO - Iter [32900/40000]	lr: 1.065e-05, eta: 0:34:11, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.3910, decode.acc_seg: 79.9175, loss: 0.3910
2022-11-29 19:08:27,742 - mmseg - INFO - Iter [32950/40000]	lr: 1.058e-05, eta: 0:33:56, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1950, decode.acc_seg: 88.3022, loss: 0.1950
2022-11-29 19:08:40,818 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:08:40,818 - mmseg - INFO - Iter [33000/40000]	lr: 1.050e-05, eta: 0:33:41, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2514, decode.acc_seg: 87.2031, loss: 0.2514
2022-11-29 19:08:53,889 - mmseg - INFO - Iter [33050/40000]	lr: 1.043e-05, eta: 0:33:27, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2203, decode.acc_seg: 88.2552, loss: 0.2203
2022-11-29 19:09:07,022 - mmseg - INFO - Iter [33100/40000]	lr: 1.035e-05, eta: 0:33:12, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1735, decode.acc_seg: 88.6070, loss: 0.1735
2022-11-29 19:09:20,068 - mmseg - INFO - Iter [33150/40000]	lr: 1.028e-05, eta: 0:32:57, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2072, decode.acc_seg: 87.5826, loss: 0.2072
2022-11-29 19:09:33,105 - mmseg - INFO - Iter [33200/40000]	lr: 1.020e-05, eta: 0:32:42, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1552, decode.acc_seg: 91.2766, loss: 0.1552
2022-11-29 19:09:46,220 - mmseg - INFO - Iter [33250/40000]	lr: 1.013e-05, eta: 0:32:28, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2319, decode.acc_seg: 85.5257, loss: 0.2319
2022-11-29 19:09:59,273 - mmseg - INFO - Iter [33300/40000]	lr: 1.005e-05, eta: 0:32:13, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1405, decode.acc_seg: 90.6089, loss: 0.1405
2022-11-29 19:10:12,343 - mmseg - INFO - Iter [33350/40000]	lr: 9.976e-06, eta: 0:31:58, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2122, decode.acc_seg: 86.6638, loss: 0.2122
2022-11-29 19:10:25,411 - mmseg - INFO - Iter [33400/40000]	lr: 9.901e-06, eta: 0:31:44, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2129, decode.acc_seg: 86.6393, loss: 0.2129
2022-11-29 19:10:38,518 - mmseg - INFO - Iter [33450/40000]	lr: 9.826e-06, eta: 0:31:29, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2221, decode.acc_seg: 85.6307, loss: 0.2221
2022-11-29 19:10:51,560 - mmseg - INFO - Iter [33500/40000]	lr: 9.752e-06, eta: 0:31:14, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1855, decode.acc_seg: 88.8368, loss: 0.1855
2022-11-29 19:11:04,618 - mmseg - INFO - Iter [33550/40000]	lr: 9.676e-06, eta: 0:31:00, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2256, decode.acc_seg: 84.8429, loss: 0.2256
2022-11-29 19:11:20,099 - mmseg - INFO - Iter [33600/40000]	lr: 9.601e-06, eta: 0:30:45, time: 0.310, data_time: 0.053, memory: 5537, decode.loss_ce: 0.1986, decode.acc_seg: 87.2592, loss: 0.1986
2022-11-29 19:11:33,220 - mmseg - INFO - Iter [33650/40000]	lr: 9.527e-06, eta: 0:30:31, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1957, decode.acc_seg: 88.0683, loss: 0.1957
2022-11-29 19:11:46,350 - mmseg - INFO - Iter [33700/40000]	lr: 9.452e-06, eta: 0:30:16, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1973, decode.acc_seg: 85.8312, loss: 0.1973
2022-11-29 19:11:59,528 - mmseg - INFO - Iter [33750/40000]	lr: 9.377e-06, eta: 0:30:01, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2057, decode.acc_seg: 88.5095, loss: 0.2057
2022-11-29 19:12:12,702 - mmseg - INFO - Iter [33800/40000]	lr: 9.301e-06, eta: 0:29:47, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1786, decode.acc_seg: 89.7218, loss: 0.1786
2022-11-29 19:12:25,741 - mmseg - INFO - Iter [33850/40000]	lr: 9.227e-06, eta: 0:29:32, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2224, decode.acc_seg: 84.1993, loss: 0.2224
2022-11-29 19:12:38,772 - mmseg - INFO - Iter [33900/40000]	lr: 9.152e-06, eta: 0:29:17, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1644, decode.acc_seg: 87.8980, loss: 0.1644
2022-11-29 19:12:51,823 - mmseg - INFO - Iter [33950/40000]	lr: 9.077e-06, eta: 0:29:03, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1949, decode.acc_seg: 88.2542, loss: 0.1949
2022-11-29 19:13:04,892 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:13:04,892 - mmseg - INFO - Iter [34000/40000]	lr: 9.001e-06, eta: 0:28:48, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1784, decode.acc_seg: 88.4547, loss: 0.1784
2022-11-29 19:13:17,946 - mmseg - INFO - Iter [34050/40000]	lr: 8.926e-06, eta: 0:28:33, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1672, decode.acc_seg: 88.3486, loss: 0.1672
2022-11-29 19:13:30,981 - mmseg - INFO - Iter [34100/40000]	lr: 8.852e-06, eta: 0:28:19, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1981, decode.acc_seg: 86.5333, loss: 0.1981
2022-11-29 19:13:44,097 - mmseg - INFO - Iter [34150/40000]	lr: 8.777e-06, eta: 0:28:04, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1680, decode.acc_seg: 89.4807, loss: 0.1680
2022-11-29 19:13:57,143 - mmseg - INFO - Iter [34200/40000]	lr: 8.701e-06, eta: 0:27:50, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2427, decode.acc_seg: 85.4016, loss: 0.2427
2022-11-29 19:14:10,172 - mmseg - INFO - Iter [34250/40000]	lr: 8.626e-06, eta: 0:27:35, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1720, decode.acc_seg: 89.3999, loss: 0.1720
2022-11-29 19:14:23,334 - mmseg - INFO - Iter [34300/40000]	lr: 8.552e-06, eta: 0:27:20, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1673, decode.acc_seg: 89.2750, loss: 0.1673
2022-11-29 19:14:36,434 - mmseg - INFO - Iter [34350/40000]	lr: 8.477e-06, eta: 0:27:06, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1697, decode.acc_seg: 91.1108, loss: 0.1697
2022-11-29 19:14:49,696 - mmseg - INFO - Iter [34400/40000]	lr: 8.401e-06, eta: 0:26:51, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1969, decode.acc_seg: 88.3176, loss: 0.1969
2022-11-29 19:15:02,733 - mmseg - INFO - Iter [34450/40000]	lr: 8.326e-06, eta: 0:26:37, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2491, decode.acc_seg: 84.9285, loss: 0.2491
2022-11-29 19:15:15,856 - mmseg - INFO - Iter [34500/40000]	lr: 8.252e-06, eta: 0:26:22, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1999, decode.acc_seg: 85.9039, loss: 0.1999
2022-11-29 19:15:31,142 - mmseg - INFO - Iter [34550/40000]	lr: 8.177e-06, eta: 0:26:08, time: 0.306, data_time: 0.047, memory: 5537, decode.loss_ce: 0.1612, decode.acc_seg: 88.9686, loss: 0.1612
2022-11-29 19:15:44,258 - mmseg - INFO - Iter [34600/40000]	lr: 8.101e-06, eta: 0:25:53, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1746, decode.acc_seg: 90.3375, loss: 0.1746
2022-11-29 19:15:57,754 - mmseg - INFO - Iter [34650/40000]	lr: 8.026e-06, eta: 0:25:39, time: 0.270, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2053, decode.acc_seg: 86.9123, loss: 0.2053
2022-11-29 19:16:10,934 - mmseg - INFO - Iter [34700/40000]	lr: 7.952e-06, eta: 0:25:24, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1845, decode.acc_seg: 89.6000, loss: 0.1845
2022-11-29 19:16:23,971 - mmseg - INFO - Iter [34750/40000]	lr: 7.877e-06, eta: 0:25:09, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2095, decode.acc_seg: 87.8552, loss: 0.2095
2022-11-29 19:16:37,018 - mmseg - INFO - Iter [34800/40000]	lr: 7.801e-06, eta: 0:24:55, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2471, decode.acc_seg: 83.1586, loss: 0.2471
2022-11-29 19:16:50,143 - mmseg - INFO - Iter [34850/40000]	lr: 7.726e-06, eta: 0:24:40, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2788, decode.acc_seg: 83.9998, loss: 0.2788
2022-11-29 19:17:03,258 - mmseg - INFO - Iter [34900/40000]	lr: 7.651e-06, eta: 0:24:26, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2126, decode.acc_seg: 86.4992, loss: 0.2126
2022-11-29 19:17:16,303 - mmseg - INFO - Iter [34950/40000]	lr: 7.577e-06, eta: 0:24:11, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2229, decode.acc_seg: 85.9911, loss: 0.2229
2022-11-29 19:17:29,436 - mmseg - INFO - Saving checkpoint at 35000 iterations
2022-11-29 19:17:32,348 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:17:32,349 - mmseg - INFO - Iter [35000/40000]	lr: 7.502e-06, eta: 0:23:57, time: 0.321, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2037, decode.acc_seg: 89.0850, loss: 0.2037
2022-11-29 19:18:55,787 - mmseg - INFO - per class results:
2022-11-29 19:18:55,788 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
| background | 56.07 | 63.19 |
|  building  | 23.87 | 27.24 |
|  woodland  | 48.08 | 54.55 |
|   water    | 13.53 | 75.85 |
|    road    | 15.52 | 20.43 |
+------------+-------+-------+
2022-11-29 19:18:55,788 - mmseg - INFO - Summary:
2022-11-29 19:18:55,788 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 60.23 | 31.41 | 48.25 |
+-------+-------+-------+
2022-11-29 19:18:55,790 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:18:55,790 - mmseg - INFO - Iter(val) [1602]	aAcc: 0.6023, mIoU: 0.3141, mAcc: 0.4825, IoU.background: 0.5607, IoU.building: 0.2387, IoU.woodland: 0.4808, IoU.water: 0.1353, IoU.road: 0.1552, Acc.background: 0.6319, Acc.building: 0.2724, Acc.woodland: 0.5455, Acc.water: 0.7585, Acc.road: 0.2043
2022-11-29 19:19:08,983 - mmseg - INFO - Iter [35050/40000]	lr: 7.426e-06, eta: 0:23:54, time: 1.933, data_time: 1.675, memory: 5537, decode.loss_ce: 0.1676, decode.acc_seg: 87.6313, loss: 0.1676
2022-11-29 19:19:22,060 - mmseg - INFO - Iter [35100/40000]	lr: 7.351e-06, eta: 0:23:40, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2019, decode.acc_seg: 86.2807, loss: 0.2019
2022-11-29 19:19:35,108 - mmseg - INFO - Iter [35150/40000]	lr: 7.277e-06, eta: 0:23:25, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1819, decode.acc_seg: 87.0852, loss: 0.1819
2022-11-29 19:19:48,293 - mmseg - INFO - Iter [35200/40000]	lr: 7.202e-06, eta: 0:23:10, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1771, decode.acc_seg: 88.9896, loss: 0.1771
2022-11-29 19:20:01,430 - mmseg - INFO - Iter [35250/40000]	lr: 7.126e-06, eta: 0:22:56, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2456, decode.acc_seg: 81.1744, loss: 0.2456
2022-11-29 19:20:14,530 - mmseg - INFO - Iter [35300/40000]	lr: 7.051e-06, eta: 0:22:41, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2501, decode.acc_seg: 85.6470, loss: 0.2501
2022-11-29 19:20:27,698 - mmseg - INFO - Iter [35350/40000]	lr: 6.977e-06, eta: 0:22:26, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1346, decode.acc_seg: 91.9969, loss: 0.1346
2022-11-29 19:20:42,867 - mmseg - INFO - Iter [35400/40000]	lr: 6.902e-06, eta: 0:22:12, time: 0.303, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1582, decode.acc_seg: 90.9942, loss: 0.1582
2022-11-29 19:20:57,201 - mmseg - INFO - Iter [35450/40000]	lr: 6.826e-06, eta: 0:21:57, time: 0.287, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1936, decode.acc_seg: 84.7744, loss: 0.1936
2022-11-29 19:21:12,507 - mmseg - INFO - Iter [35500/40000]	lr: 6.751e-06, eta: 0:21:43, time: 0.306, data_time: 0.047, memory: 5537, decode.loss_ce: 0.1532, decode.acc_seg: 90.6743, loss: 0.1532
2022-11-29 19:21:25,504 - mmseg - INFO - Iter [35550/40000]	lr: 6.677e-06, eta: 0:21:28, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2155, decode.acc_seg: 85.5710, loss: 0.2155
2022-11-29 19:21:38,566 - mmseg - INFO - Iter [35600/40000]	lr: 6.602e-06, eta: 0:21:14, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1622, decode.acc_seg: 87.8160, loss: 0.1622
2022-11-29 19:21:51,563 - mmseg - INFO - Iter [35650/40000]	lr: 6.526e-06, eta: 0:20:59, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2083, decode.acc_seg: 85.0754, loss: 0.2083
2022-11-29 19:22:04,608 - mmseg - INFO - Iter [35700/40000]	lr: 6.451e-06, eta: 0:20:44, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2061, decode.acc_seg: 86.1415, loss: 0.2061
2022-11-29 19:22:17,646 - mmseg - INFO - Iter [35750/40000]	lr: 6.377e-06, eta: 0:20:30, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1937, decode.acc_seg: 88.5693, loss: 0.1937
2022-11-29 19:22:30,707 - mmseg - INFO - Iter [35800/40000]	lr: 6.302e-06, eta: 0:20:15, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2072, decode.acc_seg: 85.6357, loss: 0.2072
2022-11-29 19:22:43,714 - mmseg - INFO - Iter [35850/40000]	lr: 6.226e-06, eta: 0:20:00, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1730, decode.acc_seg: 88.2872, loss: 0.1730
2022-11-29 19:22:56,816 - mmseg - INFO - Iter [35900/40000]	lr: 6.151e-06, eta: 0:19:46, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1409, decode.acc_seg: 91.1720, loss: 0.1409
2022-11-29 19:23:09,879 - mmseg - INFO - Iter [35950/40000]	lr: 6.077e-06, eta: 0:19:31, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2028, decode.acc_seg: 87.8087, loss: 0.2028
2022-11-29 19:23:22,921 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:23:22,921 - mmseg - INFO - Iter [36000/40000]	lr: 6.002e-06, eta: 0:19:17, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1863, decode.acc_seg: 88.8147, loss: 0.1863
2022-11-29 19:23:36,045 - mmseg - INFO - Iter [36050/40000]	lr: 5.926e-06, eta: 0:19:02, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1886, decode.acc_seg: 87.5086, loss: 0.1886
2022-11-29 19:23:49,263 - mmseg - INFO - Iter [36100/40000]	lr: 5.851e-06, eta: 0:18:47, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1969, decode.acc_seg: 89.5669, loss: 0.1969
2022-11-29 19:24:02,736 - mmseg - INFO - Iter [36150/40000]	lr: 5.777e-06, eta: 0:18:33, time: 0.269, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1615, decode.acc_seg: 87.4811, loss: 0.1615
2022-11-29 19:24:15,873 - mmseg - INFO - Iter [36200/40000]	lr: 5.702e-06, eta: 0:18:18, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2469, decode.acc_seg: 85.5537, loss: 0.2469
2022-11-29 19:24:28,892 - mmseg - INFO - Iter [36250/40000]	lr: 5.627e-06, eta: 0:18:04, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2089, decode.acc_seg: 86.6153, loss: 0.2089
2022-11-29 19:24:41,985 - mmseg - INFO - Iter [36300/40000]	lr: 5.551e-06, eta: 0:17:49, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1928, decode.acc_seg: 87.7908, loss: 0.1928
2022-11-29 19:24:55,082 - mmseg - INFO - Iter [36350/40000]	lr: 5.476e-06, eta: 0:17:34, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1925, decode.acc_seg: 86.3713, loss: 0.1925
2022-11-29 19:25:10,687 - mmseg - INFO - Iter [36400/40000]	lr: 5.402e-06, eta: 0:17:20, time: 0.312, data_time: 0.047, memory: 5537, decode.loss_ce: 0.2003, decode.acc_seg: 85.8803, loss: 0.2003
2022-11-29 19:25:24,370 - mmseg - INFO - Iter [36450/40000]	lr: 5.327e-06, eta: 0:17:06, time: 0.274, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1600, decode.acc_seg: 89.9263, loss: 0.1600
2022-11-29 19:25:38,646 - mmseg - INFO - Iter [36500/40000]	lr: 5.251e-06, eta: 0:16:51, time: 0.286, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1356, decode.acc_seg: 90.8407, loss: 0.1356
2022-11-29 19:25:51,938 - mmseg - INFO - Iter [36550/40000]	lr: 5.176e-06, eta: 0:16:37, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1769, decode.acc_seg: 87.9122, loss: 0.1769
2022-11-29 19:26:05,063 - mmseg - INFO - Iter [36600/40000]	lr: 5.102e-06, eta: 0:16:22, time: 0.262, data_time: 0.005, memory: 5537, decode.loss_ce: 0.2025, decode.acc_seg: 89.3994, loss: 0.2025
2022-11-29 19:26:18,215 - mmseg - INFO - Iter [36650/40000]	lr: 5.027e-06, eta: 0:16:07, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1899, decode.acc_seg: 87.2485, loss: 0.1899
2022-11-29 19:26:31,307 - mmseg - INFO - Iter [36700/40000]	lr: 4.951e-06, eta: 0:15:53, time: 0.262, data_time: 0.005, memory: 5537, decode.loss_ce: 0.2174, decode.acc_seg: 83.8267, loss: 0.2174
2022-11-29 19:26:44,440 - mmseg - INFO - Iter [36750/40000]	lr: 4.876e-06, eta: 0:15:38, time: 0.263, data_time: 0.005, memory: 5537, decode.loss_ce: 0.1953, decode.acc_seg: 85.4204, loss: 0.1953
2022-11-29 19:26:57,502 - mmseg - INFO - Iter [36800/40000]	lr: 4.802e-06, eta: 0:15:24, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2334, decode.acc_seg: 84.1753, loss: 0.2334
2022-11-29 19:27:10,649 - mmseg - INFO - Iter [36850/40000]	lr: 4.727e-06, eta: 0:15:09, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1725, decode.acc_seg: 89.0921, loss: 0.1725
2022-11-29 19:27:23,750 - mmseg - INFO - Iter [36900/40000]	lr: 4.651e-06, eta: 0:14:55, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1618, decode.acc_seg: 88.5209, loss: 0.1618
2022-11-29 19:27:36,878 - mmseg - INFO - Iter [36950/40000]	lr: 4.576e-06, eta: 0:14:40, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2034, decode.acc_seg: 87.9151, loss: 0.2034
2022-11-29 19:27:50,047 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:27:50,047 - mmseg - INFO - Iter [37000/40000]	lr: 4.502e-06, eta: 0:14:25, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2003, decode.acc_seg: 87.4471, loss: 0.2003
2022-11-29 19:28:03,198 - mmseg - INFO - Iter [37050/40000]	lr: 4.427e-06, eta: 0:14:11, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2495, decode.acc_seg: 86.2997, loss: 0.2495
2022-11-29 19:28:16,332 - mmseg - INFO - Iter [37100/40000]	lr: 4.351e-06, eta: 0:13:56, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1926, decode.acc_seg: 87.6778, loss: 0.1926
2022-11-29 19:28:29,438 - mmseg - INFO - Iter [37150/40000]	lr: 4.276e-06, eta: 0:13:42, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1850, decode.acc_seg: 87.1064, loss: 0.1850
2022-11-29 19:28:42,901 - mmseg - INFO - Iter [37200/40000]	lr: 4.202e-06, eta: 0:13:27, time: 0.269, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1719, decode.acc_seg: 89.3307, loss: 0.1719
2022-11-29 19:28:56,003 - mmseg - INFO - Iter [37250/40000]	lr: 4.127e-06, eta: 0:13:13, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1623, decode.acc_seg: 88.3144, loss: 0.1623
2022-11-29 19:29:09,119 - mmseg - INFO - Iter [37300/40000]	lr: 4.051e-06, eta: 0:12:58, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2002, decode.acc_seg: 86.9235, loss: 0.2002
2022-11-29 19:29:24,363 - mmseg - INFO - Iter [37350/40000]	lr: 3.976e-06, eta: 0:12:44, time: 0.305, data_time: 0.047, memory: 5537, decode.loss_ce: 0.1775, decode.acc_seg: 87.9685, loss: 0.1775
2022-11-29 19:29:37,541 - mmseg - INFO - Iter [37400/40000]	lr: 3.901e-06, eta: 0:12:29, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1646, decode.acc_seg: 89.7448, loss: 0.1646
2022-11-29 19:29:50,580 - mmseg - INFO - Iter [37450/40000]	lr: 3.827e-06, eta: 0:12:15, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2570, decode.acc_seg: 85.2761, loss: 0.2570
2022-11-29 19:30:03,627 - mmseg - INFO - Iter [37500/40000]	lr: 3.752e-06, eta: 0:12:00, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2239, decode.acc_seg: 87.1672, loss: 0.2239
2022-11-29 19:30:16,741 - mmseg - INFO - Iter [37550/40000]	lr: 3.676e-06, eta: 0:11:46, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1892, decode.acc_seg: 88.5472, loss: 0.1892
2022-11-29 19:30:29,863 - mmseg - INFO - Iter [37600/40000]	lr: 3.601e-06, eta: 0:11:31, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2251, decode.acc_seg: 82.3830, loss: 0.2251
2022-11-29 19:30:42,980 - mmseg - INFO - Iter [37650/40000]	lr: 3.527e-06, eta: 0:11:17, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1376, decode.acc_seg: 91.2459, loss: 0.1376
2022-11-29 19:30:56,076 - mmseg - INFO - Iter [37700/40000]	lr: 3.452e-06, eta: 0:11:02, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1733, decode.acc_seg: 90.2908, loss: 0.1733
2022-11-29 19:31:09,149 - mmseg - INFO - Iter [37750/40000]	lr: 3.376e-06, eta: 0:10:48, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2366, decode.acc_seg: 87.8547, loss: 0.2366
2022-11-29 19:31:22,179 - mmseg - INFO - Iter [37800/40000]	lr: 3.301e-06, eta: 0:10:33, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1708, decode.acc_seg: 87.4535, loss: 0.1708
2022-11-29 19:31:35,210 - mmseg - INFO - Iter [37850/40000]	lr: 3.227e-06, eta: 0:10:19, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2599, decode.acc_seg: 86.1143, loss: 0.2599
2022-11-29 19:31:48,320 - mmseg - INFO - Iter [37900/40000]	lr: 3.152e-06, eta: 0:10:05, time: 0.262, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1413, decode.acc_seg: 89.5856, loss: 0.1413
2022-11-29 19:32:01,382 - mmseg - INFO - Iter [37950/40000]	lr: 3.076e-06, eta: 0:09:50, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1950, decode.acc_seg: 86.1285, loss: 0.1950
2022-11-29 19:32:14,584 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:32:14,584 - mmseg - INFO - Iter [38000/40000]	lr: 3.001e-06, eta: 0:09:36, time: 0.264, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1783, decode.acc_seg: 88.1743, loss: 0.1783
2022-11-29 19:32:27,929 - mmseg - INFO - Iter [38050/40000]	lr: 2.927e-06, eta: 0:09:21, time: 0.267, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1729, decode.acc_seg: 88.3514, loss: 0.1729
2022-11-29 19:32:42,125 - mmseg - INFO - Iter [38100/40000]	lr: 2.852e-06, eta: 0:09:07, time: 0.284, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1868, decode.acc_seg: 89.7255, loss: 0.1868
2022-11-29 19:32:55,208 - mmseg - INFO - Iter [38150/40000]	lr: 2.776e-06, eta: 0:08:52, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1651, decode.acc_seg: 89.4731, loss: 0.1651
2022-11-29 19:33:08,236 - mmseg - INFO - Iter [38200/40000]	lr: 2.701e-06, eta: 0:08:38, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1933, decode.acc_seg: 87.3264, loss: 0.1933
2022-11-29 19:33:21,309 - mmseg - INFO - Iter [38250/40000]	lr: 2.627e-06, eta: 0:08:23, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1534, decode.acc_seg: 89.0404, loss: 0.1534
2022-11-29 19:33:39,055 - mmseg - INFO - Iter [38300/40000]	lr: 2.552e-06, eta: 0:08:09, time: 0.355, data_time: 0.048, memory: 5537, decode.loss_ce: 0.2247, decode.acc_seg: 86.7116, loss: 0.2247
2022-11-29 19:33:52,644 - mmseg - INFO - Iter [38350/40000]	lr: 2.476e-06, eta: 0:07:55, time: 0.272, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1598, decode.acc_seg: 88.7342, loss: 0.1598
2022-11-29 19:34:05,697 - mmseg - INFO - Iter [38400/40000]	lr: 2.401e-06, eta: 0:07:40, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1811, decode.acc_seg: 87.0881, loss: 0.1811
2022-11-29 19:34:18,855 - mmseg - INFO - Iter [38450/40000]	lr: 2.327e-06, eta: 0:07:26, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2156, decode.acc_seg: 88.1810, loss: 0.2156
2022-11-29 19:34:31,900 - mmseg - INFO - Iter [38500/40000]	lr: 2.252e-06, eta: 0:07:11, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2127, decode.acc_seg: 86.8908, loss: 0.2127
2022-11-29 19:34:44,967 - mmseg - INFO - Iter [38550/40000]	lr: 2.176e-06, eta: 0:06:57, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1919, decode.acc_seg: 88.8591, loss: 0.1919
2022-11-29 19:34:58,114 - mmseg - INFO - Iter [38600/40000]	lr: 2.101e-06, eta: 0:06:42, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1820, decode.acc_seg: 87.1247, loss: 0.1820
2022-11-29 19:35:11,268 - mmseg - INFO - Iter [38650/40000]	lr: 2.026e-06, eta: 0:06:28, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1725, decode.acc_seg: 88.3917, loss: 0.1725
2022-11-29 19:35:24,377 - mmseg - INFO - Iter [38700/40000]	lr: 1.952e-06, eta: 0:06:14, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1284, decode.acc_seg: 90.3614, loss: 0.1284
2022-11-29 19:35:37,452 - mmseg - INFO - Iter [38750/40000]	lr: 1.877e-06, eta: 0:05:59, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1501, decode.acc_seg: 89.0315, loss: 0.1501
2022-11-29 19:35:50,567 - mmseg - INFO - Iter [38800/40000]	lr: 1.801e-06, eta: 0:05:45, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1950, decode.acc_seg: 87.3668, loss: 0.1950
2022-11-29 19:36:03,671 - mmseg - INFO - Iter [38850/40000]	lr: 1.726e-06, eta: 0:05:30, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1622, decode.acc_seg: 89.1079, loss: 0.1622
2022-11-29 19:36:16,844 - mmseg - INFO - Iter [38900/40000]	lr: 1.652e-06, eta: 0:05:16, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1455, decode.acc_seg: 89.7571, loss: 0.1455
2022-11-29 19:36:29,972 - mmseg - INFO - Iter [38950/40000]	lr: 1.577e-06, eta: 0:05:01, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2089, decode.acc_seg: 85.4158, loss: 0.2089
2022-11-29 19:36:43,160 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:36:43,160 - mmseg - INFO - Iter [39000/40000]	lr: 1.501e-06, eta: 0:04:47, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1760, decode.acc_seg: 87.7189, loss: 0.1760
2022-11-29 19:36:56,183 - mmseg - INFO - Iter [39050/40000]	lr: 1.426e-06, eta: 0:04:33, time: 0.260, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1445, decode.acc_seg: 91.7087, loss: 0.1445
2022-11-29 19:37:09,226 - mmseg - INFO - Iter [39100/40000]	lr: 1.352e-06, eta: 0:04:18, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1798, decode.acc_seg: 86.7216, loss: 0.1798
2022-11-29 19:37:22,324 - mmseg - INFO - Iter [39150/40000]	lr: 1.277e-06, eta: 0:04:04, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2045, decode.acc_seg: 85.9722, loss: 0.2045
2022-11-29 19:37:37,486 - mmseg - INFO - Iter [39200/40000]	lr: 1.201e-06, eta: 0:03:49, time: 0.303, data_time: 0.047, memory: 5537, decode.loss_ce: 0.1867, decode.acc_seg: 88.1023, loss: 0.1867
2022-11-29 19:37:50,523 - mmseg - INFO - Iter [39250/40000]	lr: 1.126e-06, eta: 0:03:35, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1719, decode.acc_seg: 88.5863, loss: 0.1719
2022-11-29 19:38:03,552 - mmseg - INFO - Iter [39300/40000]	lr: 1.052e-06, eta: 0:03:21, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1900, decode.acc_seg: 87.3212, loss: 0.1900
2022-11-29 19:38:17,164 - mmseg - INFO - Iter [39350/40000]	lr: 9.765e-07, eta: 0:03:06, time: 0.272, data_time: 0.007, memory: 5537, decode.loss_ce: 0.1727, decode.acc_seg: 89.4264, loss: 0.1727
2022-11-29 19:38:30,284 - mmseg - INFO - Iter [39400/40000]	lr: 9.015e-07, eta: 0:02:52, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1481, decode.acc_seg: 91.0848, loss: 0.1481
2022-11-29 19:38:43,400 - mmseg - INFO - Iter [39450/40000]	lr: 8.265e-07, eta: 0:02:38, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2390, decode.acc_seg: 85.8307, loss: 0.2390
2022-11-29 19:38:56,519 - mmseg - INFO - Iter [39500/40000]	lr: 7.515e-07, eta: 0:02:23, time: 0.262, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1590, decode.acc_seg: 91.1815, loss: 0.1590
2022-11-29 19:39:09,654 - mmseg - INFO - Iter [39550/40000]	lr: 6.765e-07, eta: 0:02:09, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1867, decode.acc_seg: 86.4812, loss: 0.1867
2022-11-29 19:39:22,782 - mmseg - INFO - Iter [39600/40000]	lr: 6.015e-07, eta: 0:01:54, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1591, decode.acc_seg: 90.0398, loss: 0.1591
2022-11-29 19:39:35,835 - mmseg - INFO - Iter [39650/40000]	lr: 5.265e-07, eta: 0:01:40, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2135, decode.acc_seg: 85.9185, loss: 0.2135
2022-11-29 19:39:48,909 - mmseg - INFO - Iter [39700/40000]	lr: 4.515e-07, eta: 0:01:26, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1575, decode.acc_seg: 91.9641, loss: 0.1575
2022-11-29 19:40:01,980 - mmseg - INFO - Iter [39750/40000]	lr: 3.765e-07, eta: 0:01:11, time: 0.261, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1886, decode.acc_seg: 87.0112, loss: 0.1886
2022-11-29 19:40:15,128 - mmseg - INFO - Iter [39800/40000]	lr: 3.015e-07, eta: 0:00:57, time: 0.263, data_time: 0.006, memory: 5537, decode.loss_ce: 0.1791, decode.acc_seg: 89.2996, loss: 0.1791
2022-11-29 19:40:28,393 - mmseg - INFO - Iter [39850/40000]	lr: 2.265e-07, eta: 0:00:43, time: 0.265, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2024, decode.acc_seg: 85.7895, loss: 0.2024
2022-11-29 19:40:41,606 - mmseg - INFO - Iter [39900/40000]	lr: 1.515e-07, eta: 0:00:28, time: 0.264, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2137, decode.acc_seg: 87.9093, loss: 0.2137
2022-11-29 19:40:54,898 - mmseg - INFO - Iter [39950/40000]	lr: 7.650e-08, eta: 0:00:14, time: 0.266, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2209, decode.acc_seg: 83.8992, loss: 0.2209
2022-11-29 19:41:07,990 - mmseg - INFO - Saving checkpoint at 40000 iterations
2022-11-29 19:41:10,272 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:41:10,273 - mmseg - INFO - Iter [40000/40000]	lr: 1.500e-09, eta: 0:00:00, time: 0.307, data_time: 0.006, memory: 5537, decode.loss_ce: 0.2027, decode.acc_seg: 88.2727, loss: 0.2027
2022-11-29 19:42:31,056 - mmseg - INFO - per class results:
2022-11-29 19:42:31,056 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
| background | 51.65 | 57.85 |
|  building  | 22.35 | 24.53 |
|  woodland  | 42.05 |  46.4 |
|   water    |  12.1 | 82.19 |
|    road    | 14.12 | 17.65 |
+------------+-------+-------+
2022-11-29 19:42:31,057 - mmseg - INFO - Summary:
2022-11-29 19:42:31,057 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 54.76 | 28.45 | 45.72 |
+-------+-------+-------+
2022-11-29 19:42:31,059 - mmseg - INFO - Exp name: segnext.large.512x512.landcover.40k_weight_sub_nopre.py
2022-11-29 19:42:31,059 - mmseg - INFO - Iter(val) [1602]	aAcc: 0.5476, mIoU: 0.2845, mAcc: 0.4572, IoU.background: 0.5165, IoU.building: 0.2235, IoU.woodland: 0.4205, IoU.water: 0.1210, IoU.road: 0.1412, Acc.background: 0.5785, Acc.building: 0.2453, Acc.woodland: 0.4640, Acc.water: 0.8219, Acc.road: 0.1765
